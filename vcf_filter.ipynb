{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omar-Abd-Elwahab/VCF_fiilter/blob/main/vcf_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBf39uImEzXe",
        "outputId": "3241c664-84c3-4361-b5f0-83ef23bf8aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Opening the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVaakIbME0fw"
      },
      "outputs": [],
      "source": [
        "!cp ./drive/MyDrive/Colab\\ Notebooks/deepref/hg003_snps.csv /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9iHfNBeE0jq",
        "outputId": "2e2eaae9-c45d-4e7f-a8bf-03c2943ec177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  hg003_snps.csv  \u001b[01;34msample_data\u001b[0m/  unique_words.txt\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SAjl3ZeRD-Su",
        "outputId": "7cba82fe-61ca-47e0-f922-a67d6b29d18f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "wB6pVNDjFaBq",
        "outputId": "83a55916-eddf-4a35-82b5-49876d26b37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training lines: 12,776,714\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4bee30f3-1191-49ff-830f-8d3ef9a784d9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALT</th>\n",
              "      <th>QUAL</th>\n",
              "      <th>FILTER</th>\n",
              "      <th>INFO</th>\n",
              "      <th>FORMAT</th>\n",
              "      <th>HG003</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7214250</th>\n",
              "      <td>T</td>\n",
              "      <td>100.028</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=8;VDB=0.145712;RPBZ=-1.0435;BQBZ=1.29099;SC...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>0/1:133,0,77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10358951</th>\n",
              "      <td>C</td>\n",
              "      <td>270.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=-0.055;DP=15;Exc...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:5,9:14:99:278,0,137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11423790</th>\n",
              "      <td>T</td>\n",
              "      <td>138.64</td>\n",
              "      <td>1</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=-0.108;DP=8;Exce...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:3,4:7:93:146,0,93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bee30f3-1191-49ff-830f-8d3ef9a784d9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4bee30f3-1191-49ff-830f-8d3ef9a784d9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4bee30f3-1191-49ff-830f-8d3ef9a784d9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bb0f1aeb-76d9-4f0b-a044-1b058854b442\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb0f1aeb-76d9-4f0b-a044-1b058854b442')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bb0f1aeb-76d9-4f0b-a044-1b058854b442 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         ALT     QUAL FILTER  \\\n",
              "7214250    T  100.028      0   \n",
              "10358951   C   270.64      0   \n",
              "11423790   T   138.64      1   \n",
              "\n",
              "                                                       INFO          FORMAT  \\\n",
              "7214250   DP=8;VDB=0.145712;RPBZ=-1.0435;BQBZ=1.29099;SC...           GT:PL   \n",
              "10358951  AC=1;AF=0.5;AN=2;BaseQRankSum=-0.055;DP=15;Exc...  GT:AD:DP:GQ:PL   \n",
              "11423790  AC=1;AF=0.5;AN=2;BaseQRankSum=-0.108;DP=8;Exce...  GT:AD:DP:GQ:PL   \n",
              "\n",
              "                            HG003  \n",
              "7214250              0/1:133,0,77  \n",
              "10358951  0/1:5,9:14:99:278,0,137  \n",
              "11423790    0/1:3,4:7:93:146,0,93  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"hg003_snps.csv\", low_memory=False)\n",
        "df=df.drop(df.columns[[0, 1]], axis=1)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training lines: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyboLnKEyfc"
      },
      "outputs": [],
      "source": [
        "df=df[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9BXxGMIhTQv",
        "outputId": "23495217-a94d-42cb-acea-c160e1711f10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FILTER\n",
              "0    9808038\n",
              "1    2968675\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby('FILTER').size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Sg5_-N4C8hue",
        "outputId": "2f85b597-d6e1-4c96-df7d-55e006560eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training lines: 1,000,000\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-241dc411-8a9a-45f3-a2de-1a7e01f1719a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ALT</th>\n",
              "      <th>QUAL</th>\n",
              "      <th>FILTER</th>\n",
              "      <th>INFO</th>\n",
              "      <th>FORMAT</th>\n",
              "      <th>HG003</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7141022</th>\n",
              "      <td>T</td>\n",
              "      <td>225.417</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=11;VDB=0.491564;FS=0.0;SGB=-0.676189;MQ0F=0...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:255,33,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10297984</th>\n",
              "      <td>A</td>\n",
              "      <td>102.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=0.0;DP=6;ExcessH...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:3,3:6:99:110,0,110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10324035</th>\n",
              "      <td>C</td>\n",
              "      <td>209.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=1.969;DP=13;Exce...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:7,6:13:99:217,0,230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10097077</th>\n",
              "      <td>T</td>\n",
              "      <td>241.97</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=2;AF=1.0;AN=2;DP=7;ExcessHet=0.0;FS=0.0;MLE...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>1/1:0,6:6:18:256,18,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9117347</th>\n",
              "      <td>C</td>\n",
              "      <td>288.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=-1.02;DP=15;Exce...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:6,8:14:99:296,0,214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8418395</th>\n",
              "      <td>C</td>\n",
              "      <td>167.416</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=8;VDB=0.0092202;FS=0.0;SGB=-0.651104;MQ0F=0...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:197,24,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9280669</th>\n",
              "      <td>C</td>\n",
              "      <td>522.06</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=2;AF=1.0;AN=2;DP=14;ExcessHet=0.0;FS=0.0;ML...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>1/1:0,13:13:39:536,39,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3704323</th>\n",
              "      <td>C</td>\n",
              "      <td>129.13</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=-0.04;ClippingRa...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:6,4:10:99:130,0,184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3813608</th>\n",
              "      <td>C</td>\n",
              "      <td>521.13</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=2;AF=1.0;AN=2;DP=13;Entropy=0.43;ExcessHet=...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>1/1:0,13:13:39:522,39,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8190261</th>\n",
              "      <td>T</td>\n",
              "      <td>80.2033</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=7;VDB=0.314313;RPBZ=0.0;BQBZ=1.36533;SCBZ=-...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>0/1:113,0,72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-241dc411-8a9a-45f3-a2de-1a7e01f1719a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-241dc411-8a9a-45f3-a2de-1a7e01f1719a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-241dc411-8a9a-45f3-a2de-1a7e01f1719a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-736ef484-3b7a-416d-8fdf-7c56aaccc764\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-736ef484-3b7a-416d-8fdf-7c56aaccc764')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-736ef484-3b7a-416d-8fdf-7c56aaccc764 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         ALT     QUAL FILTER  \\\n",
              "7141022    T  225.417      0   \n",
              "10297984   A   102.64      0   \n",
              "10324035   C   209.64      0   \n",
              "10097077   T   241.97      0   \n",
              "9117347    C   288.64      0   \n",
              "8418395    C  167.416      0   \n",
              "9280669    C   522.06      0   \n",
              "3704323    C   129.13      0   \n",
              "3813608    C   521.13      0   \n",
              "8190261    T  80.2033      0   \n",
              "\n",
              "                                                       INFO          FORMAT  \\\n",
              "7141022   DP=11;VDB=0.491564;FS=0.0;SGB=-0.676189;MQ0F=0...           GT:PL   \n",
              "10297984  AC=1;AF=0.5;AN=2;BaseQRankSum=0.0;DP=6;ExcessH...  GT:AD:DP:GQ:PL   \n",
              "10324035  AC=1;AF=0.5;AN=2;BaseQRankSum=1.969;DP=13;Exce...  GT:AD:DP:GQ:PL   \n",
              "10097077  AC=2;AF=1.0;AN=2;DP=7;ExcessHet=0.0;FS=0.0;MLE...  GT:AD:DP:GQ:PL   \n",
              "9117347   AC=1;AF=0.5;AN=2;BaseQRankSum=-1.02;DP=15;Exce...  GT:AD:DP:GQ:PL   \n",
              "8418395   DP=8;VDB=0.0092202;FS=0.0;SGB=-0.651104;MQ0F=0...           GT:PL   \n",
              "9280669   AC=2;AF=1.0;AN=2;DP=14;ExcessHet=0.0;FS=0.0;ML...  GT:AD:DP:GQ:PL   \n",
              "3704323   AC=1;AF=0.5;AN=2;BaseQRankSum=-0.04;ClippingRa...  GT:AD:DP:GQ:PL   \n",
              "3813608   AC=2;AF=1.0;AN=2;DP=13;Entropy=0.43;ExcessHet=...  GT:AD:DP:GQ:PL   \n",
              "8190261   DP=7;VDB=0.314313;RPBZ=0.0;BQBZ=1.36533;SCBZ=-...           GT:PL   \n",
              "\n",
              "                            HG003  \n",
              "7141022              1/1:255,33,0  \n",
              "10297984   0/1:3,3:6:99:110,0,110  \n",
              "10324035  0/1:7,6:13:99:217,0,230  \n",
              "10097077    1/1:0,6:6:18:256,18,0  \n",
              "9117347   0/1:6,8:14:99:296,0,214  \n",
              "8418395              1/1:197,24,0  \n",
              "9280669   1/1:0,13:13:39:536,39,0  \n",
              "3704323   0/1:6,4:10:99:130,0,184  \n",
              "3813608   1/1:0,13:13:39:522,39,0  \n",
              "8190261              0/1:113,0,72  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.sample(1000000, random_state=42)\n",
        "# Report the number of sentences.\n",
        "print('Number of training lines: {:,}\\n'.format(df.shape[0]))\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XmnnPRXJttH",
        "outputId": "68a3182b-d7c3-4054-e58e-a032db7c3eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         FILTER                                      merged_column\n",
            "5198783       0  G 46.272 DP=7;VDB=0.748679;RPBZ=1.06066;BQBZ=-...\n",
            "10997006      0  A 560.06 AC=2;AF=1.0;AN=2;DP=15;ExcessHet=0.0;...\n",
            "279544        1  T 109.13 AC=1;AF=0.5;AN=2;BaseQRankSum=-0.319;...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming you have a DataFrame called 'df' with columns 'col1', 'col2', 'col3', and 'col_to_exclude'\n",
        "\n",
        "# Get all column names except the one to exclude\n",
        "columns_to_merge = [col for col in df.columns if col != 'FILTER']\n",
        "\n",
        "# Define a lambda function to merge the values of selected columns\n",
        "merge_columns = lambda row: ' '.join(str(row[col]) for col in columns_to_merge)\n",
        "\n",
        "# Merge columns using the defined function\n",
        "df['merged_column'] = df.apply(merge_columns, axis=1)\n",
        "\n",
        "# Drop the original columns\n",
        "df = df.drop(columns=columns_to_merge)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df.sample(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3wGS_rE67m",
        "outputId": "2418ad8a-12da-43a6-8e16-6e4e93bbd0c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWd_GQwoE_J7",
        "outputId": "41958476-1a10-4a0e-de7f-b49321889031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyrcf3qnEzhE",
        "outputId": "1ac55af4-9b87-4015-dc3b-32095e40be40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T7mp_2zs2qB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2klGRY62hKRi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel, BertTokenizer,  BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer(\"unique_words.txt\", do_lower_case = True,\n",
        "                          do_basic_tokenize = True, never_split = None,\n",
        "                          unk_token = '[UNK]', sep_token = '[SEP]', pad_token = '[PAD]',\n",
        "                          cls_token = '[CLS]', mask_token = '[MASK]',\n",
        "                          tokenize_chinese_chars = False, strip_accents = None)\n",
        "config = BertConfig(vocab_size = len(tokenizer), hidden_size = 768,\n",
        "                           num_hidden_layers = 6, num_attention_heads = 6,\n",
        "                           intermediate_size = 3072, hidden_act = 'sigmoid',\n",
        "                           hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1,\n",
        "                           max_position_embeddings = 512,\n",
        "                           initializer_range = 0.02, layer_norm_eps = 1e-12,\n",
        "                           pad_token_id = 0, position_embedding_type = 'absolute',\n",
        "                           use_cache = False, classifier_dropout = None, num_labels=2,\n",
        "                           output_attentions = False, # Whether the model returns attentions weights.\n",
        "                           output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "model =  BertForSequenceClassification(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQfTYjVsOa98"
      },
      "outputs": [],
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.merged_column.values\n",
        "labels = df.FILTER.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR9LrVckQNJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = labels.astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8hgclWhOkDI",
        "outputId": "30c13842-1a3f-46fa-fe53-1b6cb317ce31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  T 67.2291 DP=11;VDB=0.148314;RPBZ=-1.82574;BQBZ=-2.9469;SCBZ=0.270801;FS=0.0;SGB=-0.590765;MQ0F=0.0;AC=1;AN=2;DP4=4,2,3,2;MQ=60 GT:PL 0/1:100,0,154\n",
            "Tokenized:  ['t', '67', '.', '229', '##1', 'dp', '=', '11', ';', 'vdb', '=', '0', '.', '148', '##31', '##4', ';', 'rpbz', '=', '-', '1', '.', '82', '##57', '##4', ';', 'bqbz', '=', '-', '2', '.', '94', '##6', '##9', ';', 'scbz', '=', '0', '.', '270', '##80', '##1', ';', 'fs', '=', '0', '.', '0', ';', 'sgb', '=', '-', '0', '.', '590', '##7', '##65', ';', 'mq', '##0', '##f', '=', '0', '.', '0', ';', 'ac', '=', '1', ';', 'an', '=', '2', ';', 'dp', '##4', '=', '4', ',', '2', ',', '3', ',', '2', ';', 'mq', '=', '60', 'gt', ':', 'pl', '0', '/', '1', ':', '100', ',', '0', ',', '154']\n",
            "Token IDs:  [1056, 6163, 1012, 22777, 2487, 30529, 1027, 2340, 1025, 30615, 1027, 1014, 1012, 16459, 21486, 2549, 1025, 30611, 1027, 1011, 1015, 1012, 6445, 28311, 2549, 1025, 30612, 1027, 1011, 1016, 1012, 6365, 2575, 2683, 1025, 30613, 1027, 1014, 1012, 13756, 17914, 2487, 1025, 30551, 1027, 1014, 1012, 1014, 1025, 30614, 1027, 1011, 1014, 1012, 25186, 2581, 26187, 1025, 30545, 2692, 2546, 1027, 1014, 1012, 1014, 1025, 9353, 1027, 1015, 1025, 2019, 1027, 1016, 1025, 30529, 2549, 1027, 1018, 1010, 1016, 1010, 1017, 1010, 1016, 1025, 30545, 1027, 3438, 14181, 1024, 20228, 1014, 1013, 1015, 1024, 2531, 1010, 1014, 1010, 16666]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebphJP8vbQee",
        "outputId": "52677f9c-017a-4f1a-80f1-e5f4b05eb885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  T 67.2291 DP=11;VDB=0.148314;RPBZ=-1.82574;BQBZ=-2.9469;SCBZ=0.270801;FS=0.0;SGB=-0.590765;MQ0F=0.0;AC=1;AN=2;DP4=4,2,3,2;MQ=60 GT:PL 0/1:100,0,154\n",
            "Token IDs: [101, 1056, 6163, 1012, 22777, 2487, 30529, 1027, 2340, 1025, 30615, 1027, 1014, 1012, 16459, 21486, 2549, 1025, 30611, 1027, 1011, 1015, 1012, 6445, 28311, 2549, 1025, 30612, 1027, 1011, 1016, 1012, 6365, 2575, 2683, 1025, 30613, 1027, 1014, 1012, 13756, 17914, 2487, 1025, 30551, 1027, 1014, 1012, 1014, 1025, 30614, 1027, 1011, 1014, 1012, 25186, 2581, 26187, 1025, 30545, 2692, 2546, 1027, 1014, 1012, 1014, 1025, 9353, 1027, 1015, 1025, 2019, 1027, 1016, 1025, 30529, 2549, 1027, 1018, 1010, 1016, 1010, 1017, 1010, 1016, 1025, 30545, 1027, 3438, 14181, 1024, 20228, 1014, 1013, 1015, 1024, 2531, 1010, 1014, 1010, 16666, 102]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEZV3QpIhWYC",
        "outputId": "25262ba0-4599-4ec8-c84e-d800bf4a7d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  238\n"
          ]
        }
      ],
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsahMGf5w6u1"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPSvdKHFwr4c",
        "outputId": "2e6ff5f6-9166-4611-f3a7-a0895120ed05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 256 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 256\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQC1ty7rw54x"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rpZEGXpwyL3"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 80% for training and 20% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=42, test_size=0.3)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76l9HQhm6s71"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YTT7Ce16s-X"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LTJ6YC6tAw",
        "outputId": "948d13d6-0687-4567-b833-0183b3ee8f0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30616, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): Sigmoid()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "#model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNuTrzjd6tDL",
        "outputId": "f99d894c-deef-4f8b-bc15-59632a6b7f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BERT model has 105 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30616, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCoEms-O6tFB",
        "outputId": "9d0bfac7-a9d0-4b14-828a-ca949418813c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv4ck8Kx68Iu"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOOUWZC368Lh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqEyOq7_68NX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6yjqF1g7V_2",
        "outputId": "f57fcbb4-13f9-4b31-a492-343aada7f8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:08.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:15.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:22.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:42.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:49.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:56.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:03.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:10.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:17.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:46.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:53.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:19:00.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:07.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:14.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:21.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:28.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:35.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:41.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:48.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:55.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:02.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:09.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:16.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:23.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:30.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:37.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:43.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:50.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:57.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:04.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:45.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:52.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:59.\n",
            "\n",
            "  Average training loss: 0.32466\n",
            "  Training epcoh took: 0:48:50\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89679\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:35.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:20.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:27.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:34.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:08.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:15.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:22.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:29.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:36.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:43.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:10.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:17.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:24.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:31.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:38.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:53.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:48:00.\n",
            "\n",
            "  Average training loss: 0.27221\n",
            "  Training epcoh took: 0:48:50\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90633\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:35.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:08.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:15.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:10.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:58.\n",
            "\n",
            "  Average training loss: 0.26226\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90804\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:30.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:25.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:28:59.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:06.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:15.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:10.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:17.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:58.\n",
            "\n",
            "  Average training loss: 0.25793\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90650\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:25.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:28:59.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:06.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:13.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:20.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:26.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:33.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:40.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:47.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:54.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:01.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:08.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:14.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:21.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:28.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:35.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:42.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:49.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:56.\n",
            "\n",
            "  Average training loss: 0.25485\n",
            "  Training epcoh took: 0:48:46\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90901\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:00.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:55.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:38.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:45.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:52.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:59.\n",
            "\n",
            "  Average training loss: 0.25212\n",
            "  Training epcoh took: 0:48:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90994\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:22.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:17.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:24.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:00.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:55.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:50.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:57.\n",
            "\n",
            "  Average training loss: 0.25002\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91058\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:43.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:50.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:42.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:49.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:56.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:02.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:09.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:16.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:23.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:30.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:37.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:44.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:51.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:57.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:04.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:52.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:59.\n",
            "\n",
            "  Average training loss: 0.24783\n",
            "  Training epcoh took: 0:48:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90945\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:24.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:58.\n",
            "\n",
            "  Average training loss: 0.24593\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91093\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:12.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:08.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:57.\n",
            "\n",
            "  Average training loss: 0.24414\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91076\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:43.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:50.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:44.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:50.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:57.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:04.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:52.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:59.\n",
            "\n",
            "  Average training loss: 0.24207\n",
            "  Training epcoh took: 0:48:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91039\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:17.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:24.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:28:59.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:06.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:12.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:19.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:26.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:33.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:40.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:47.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:54.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:01.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:08.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:14.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:21.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:28.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:35.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:42.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:49.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:56.\n",
            "\n",
            "  Average training loss: 0.24042\n",
            "  Training epcoh took: 0:48:46\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91146\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:24.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:57.\n",
            "\n",
            "  Average training loss: 0.23871\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91084\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:29.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:24.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:31.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:28:59.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:06.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:13.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:19.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:26.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:33.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:40.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:47.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:57.\n",
            "\n",
            "  Average training loss: 0.23687\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91133\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:12.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:37.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:44.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:50.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:57.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:04.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:45.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:52.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:59.\n",
            "\n",
            "  Average training loss: 0.23531\n",
            "  Training epcoh took: 0:48:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91104\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:55.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:02.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:09.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:16.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:23.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:30.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:36.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:43.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:50.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:57.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:04.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:11.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:18.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:25.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:32.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:38.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:45.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:52.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:28:59.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:06.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:13.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:20.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:10.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:17.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:58.\n",
            "\n",
            "  Average training loss: 0.23367\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91050\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:16.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:22.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:43.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:50.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:20.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:28.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:35.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:42.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:49.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:56.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:03.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:10.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:16.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:23.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:30.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:37.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:44.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:51.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:58.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:05.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:53.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:48:00.\n",
            "\n",
            "  Average training loss: 0.23201\n",
            "  Training epcoh took: 0:48:50\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91077\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:21.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:28.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:34.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:41.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:48.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:20.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:08.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:15.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:56.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:03.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:11.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:53.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:48:00.\n",
            "\n",
            "  Average training loss: 0.23059\n",
            "  Training epcoh took: 0:48:50\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91075\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:14.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:22.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:43.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:50.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:17.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:12.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:19.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:00.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:07.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:14.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:21.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:35.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:42.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:49.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:55.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:02.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:09.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:16.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:23.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:30.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:37.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:44.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:51.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:47:57.\n",
            "\n",
            "  Average training loss: 0.22901\n",
            "  Training epcoh took: 0:48:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91031\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,750.    Elapsed: 0:01:07.\n",
            "  Batch    80  of  1,750.    Elapsed: 0:02:16.\n",
            "  Batch   120  of  1,750.    Elapsed: 0:03:23.\n",
            "  Batch   160  of  1,750.    Elapsed: 0:04:29.\n",
            "  Batch   200  of  1,750.    Elapsed: 0:05:36.\n",
            "  Batch   240  of  1,750.    Elapsed: 0:06:43.\n",
            "  Batch   280  of  1,750.    Elapsed: 0:07:50.\n",
            "  Batch   320  of  1,750.    Elapsed: 0:08:57.\n",
            "  Batch   360  of  1,750.    Elapsed: 0:10:04.\n",
            "  Batch   400  of  1,750.    Elapsed: 0:11:11.\n",
            "  Batch   440  of  1,750.    Elapsed: 0:12:18.\n",
            "  Batch   480  of  1,750.    Elapsed: 0:13:24.\n",
            "  Batch   520  of  1,750.    Elapsed: 0:14:31.\n",
            "  Batch   560  of  1,750.    Elapsed: 0:15:38.\n",
            "  Batch   600  of  1,750.    Elapsed: 0:16:45.\n",
            "  Batch   640  of  1,750.    Elapsed: 0:17:52.\n",
            "  Batch   680  of  1,750.    Elapsed: 0:18:59.\n",
            "  Batch   720  of  1,750.    Elapsed: 0:20:06.\n",
            "  Batch   760  of  1,750.    Elapsed: 0:21:13.\n",
            "  Batch   800  of  1,750.    Elapsed: 0:22:20.\n",
            "  Batch   840  of  1,750.    Elapsed: 0:23:26.\n",
            "  Batch   880  of  1,750.    Elapsed: 0:24:33.\n",
            "  Batch   920  of  1,750.    Elapsed: 0:25:40.\n",
            "  Batch   960  of  1,750.    Elapsed: 0:26:47.\n",
            "  Batch 1,000  of  1,750.    Elapsed: 0:27:54.\n",
            "  Batch 1,040  of  1,750.    Elapsed: 0:29:01.\n",
            "  Batch 1,080  of  1,750.    Elapsed: 0:30:08.\n",
            "  Batch 1,120  of  1,750.    Elapsed: 0:31:15.\n",
            "  Batch 1,160  of  1,750.    Elapsed: 0:32:22.\n",
            "  Batch 1,200  of  1,750.    Elapsed: 0:33:28.\n",
            "  Batch 1,240  of  1,750.    Elapsed: 0:34:37.\n",
            "  Batch 1,280  of  1,750.    Elapsed: 0:35:44.\n",
            "  Batch 1,320  of  1,750.    Elapsed: 0:36:51.\n",
            "  Batch 1,360  of  1,750.    Elapsed: 0:37:58.\n",
            "  Batch 1,400  of  1,750.    Elapsed: 0:39:05.\n",
            "  Batch 1,440  of  1,750.    Elapsed: 0:40:12.\n",
            "  Batch 1,480  of  1,750.    Elapsed: 0:41:18.\n",
            "  Batch 1,520  of  1,750.    Elapsed: 0:42:25.\n",
            "  Batch 1,560  of  1,750.    Elapsed: 0:43:32.\n",
            "  Batch 1,600  of  1,750.    Elapsed: 0:44:39.\n",
            "  Batch 1,640  of  1,750.    Elapsed: 0:45:46.\n",
            "  Batch 1,680  of  1,750.    Elapsed: 0:46:53.\n",
            "  Batch 1,720  of  1,750.    Elapsed: 0:48:00.\n",
            "\n",
            "  Average training loss: 0.22794\n",
            "  Training epcoh took: 0:48:50\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91055\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "val_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    val_values.append(eval_accuracy/nb_eval_steps)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A5-Mz8x87WCb",
        "outputId": "939d9edf-52ec-44a0-ae83-ff43c7ff176f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "plt.plot(val_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rKkmqhnO7WEj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KKOXI_p47WGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "igsCeuiB68PI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}