{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omar-Abd-Elwahab/VCF_fiilter/blob/main/snp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBf39uImEzXe",
        "outputId": "efbdab19-3786-416f-9737-876f86d915fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Opening the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVaakIbME0fw"
      },
      "outputs": [],
      "source": [
        "!cp ./drive/MyDrive/Colab\\ Notebooks/deepref/hg003_snps.csv /content\n",
        "!cp ./drive/MyDrive/Colab\\ Notebooks/deepref/unique_words.txt /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9iHfNBeE0jq",
        "outputId": "e80b73a2-9014-41a5-e363-888f67e74506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  hg003_snps.csv  \u001b[01;34msample_data\u001b[0m/  unique_words.txt  ww.txt\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SAjl3ZeRD-Su",
        "outputId": "c31d339c-2856-4194-e997-7dc30998762a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "wB6pVNDjFaBq",
        "outputId": "8382c79c-70e1-417d-9b0f-690a66b63801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training lines: 10,787,570\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5b3b64a6-536a-47dd-ba45-1309fde62e1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>REF</th>\n",
              "      <th>ALT</th>\n",
              "      <th>QUAL</th>\n",
              "      <th>FILTER</th>\n",
              "      <th>INFO</th>\n",
              "      <th>FORMAT</th>\n",
              "      <th>HG003</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1496444</th>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>FR=1.0;MMLQ=29.0;TCR=1;HP=2;WE=4033978;Source=...</td>\n",
              "      <td>GT:GL:GOF:GQ:NR:NV</td>\n",
              "      <td>1/1:-10.2,-0.6,0.0:12.0:7:3:3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2837067</th>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>181</td>\n",
              "      <td>0</td>\n",
              "      <td>FR=0.5;MMLQ=34.0;TCR=4;HP=6;WE=46233434;Source...</td>\n",
              "      <td>GT:GL:GOF:GQ:NR:NV</td>\n",
              "      <td>1/0:-7.4,0.0,-24.3:9.0:74:13:3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9548528</th>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>66.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=0.0;DP=4;ExcessH...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:2,2:4:58:74,0,58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b3b64a6-536a-47dd-ba45-1309fde62e1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b3b64a6-536a-47dd-ba45-1309fde62e1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b3b64a6-536a-47dd-ba45-1309fde62e1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e8c13d7d-9484-4750-a999-687ce358c8e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8c13d7d-9484-4750-a999-687ce358c8e2')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e8c13d7d-9484-4750-a999-687ce358c8e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        REF ALT   QUAL FILTER  \\\n",
              "1496444   G   A     67      0   \n",
              "2837067   G   A    181      0   \n",
              "9548528   T   C  66.64      0   \n",
              "\n",
              "                                                      INFO  \\\n",
              "1496444  FR=1.0;MMLQ=29.0;TCR=1;HP=2;WE=4033978;Source=...   \n",
              "2837067  FR=0.5;MMLQ=34.0;TCR=4;HP=6;WE=46233434;Source...   \n",
              "9548528  AC=1;AF=0.5;AN=2;BaseQRankSum=0.0;DP=4;ExcessH...   \n",
              "\n",
              "                     FORMAT                           HG003  \n",
              "1496444  GT:GL:GOF:GQ:NR:NV   1/1:-10.2,-0.6,0.0:12.0:7:3:3  \n",
              "2837067  GT:GL:GOF:GQ:NR:NV  1/0:-7.4,0.0,-24.3:9.0:74:13:3  \n",
              "9548528      GT:AD:DP:GQ:PL            0/1:2,2:4:58:74,0,58  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "dat = pd.read_csv(\"hg003_snps.csv\", low_memory=False)\n",
        "dat = dat.drop(dat.columns[[0]], axis=1)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training lines: {:,}\\n'.format(dat.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "dat.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyboLnKEyfc"
      },
      "outputs": [],
      "source": [
        "dat=dat[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9BXxGMIhTQv",
        "outputId": "1d1e61c1-b00a-4c10-c62e-6a305dac7aa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FILTER\n",
              "0    9319803\n",
              "1    1467766\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat.groupby('FILTER').size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Sg5_-N4C8hue",
        "outputId": "2c5101f4-9e3f-4e05-ba98-7b6d49733abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FILTER\n",
            "0    864294\n",
            "1    135706\n",
            "dtype: int64\n",
            "Number of training lines: 1,000,000\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e5569b10-8d3a-466a-a3ea-a795a7c61956\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>REF</th>\n",
              "      <th>ALT</th>\n",
              "      <th>QUAL</th>\n",
              "      <th>FILTER</th>\n",
              "      <th>INFO</th>\n",
              "      <th>FORMAT</th>\n",
              "      <th>HG003</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2473683</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>338</td>\n",
              "      <td>0</td>\n",
              "      <td>FR=1.0;MMLQ=37.0;TCR=7;HP=2;WE=99426289;Source...</td>\n",
              "      <td>GT:GL:GOF:GQ:NR:NV</td>\n",
              "      <td>1/1:-37.3,-2.71,0.0:6.0:27:10:9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5754588</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>225.417</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=13;VDB=0.0431806;FS=0.0;SGB=-0.683931;MQ0F=...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:255,39,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5451621</th>\n",
              "      <td>T</td>\n",
              "      <td>C</td>\n",
              "      <td>123.371</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=15;VDB=0.306246;RPBZ=-1.56372;MQBZ=-1.06904...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>0/1:156,0,173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6042902</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>203.417</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=9;VDB=0.293521;MQSBZ=-1.68732;FS=0.0;SGB=-0...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:233,27,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6381692</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>34.9916</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=8;VDB=0.71601;RPBZ=0.149071;MQBZ=-0.58554;B...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>0/1:68,0,134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10105960</th>\n",
              "      <td>G</td>\n",
              "      <td>C</td>\n",
              "      <td>90.64</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=1;AF=0.5;AN=2;BaseQRankSum=0.385;DP=10;Exce...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>0/1:7,3:10:98:98,0,268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9729004</th>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>190.96</td>\n",
              "      <td>0</td>\n",
              "      <td>AC=2;AF=1.0;AN=2;DP=5;ExcessHet=0.0;FS=0.0;MLE...</td>\n",
              "      <td>GT:AD:DP:GQ:PL</td>\n",
              "      <td>1/1:0,5:5:15:205,15,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5228665</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>166.97</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=12;VDB=0.226073;RPBZ=-1.13389;BQBZ=0.755929...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>0/1:200,0,104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6544227</th>\n",
              "      <td>A</td>\n",
              "      <td>G</td>\n",
              "      <td>189.416</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=8;VDB=0.653727;FS=0.0;SGB=-0.636426;MQ0F=0....</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:219,21,0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5186841</th>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>180.416</td>\n",
              "      <td>0</td>\n",
              "      <td>DP=7;VDB=0.75529;FS=0.0;SGB=-0.636426;MQ0F=0.0...</td>\n",
              "      <td>GT:PL</td>\n",
              "      <td>1/1:210,21,0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5569b10-8d3a-466a-a3ea-a795a7c61956')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5569b10-8d3a-466a-a3ea-a795a7c61956 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5569b10-8d3a-466a-a3ea-a795a7c61956');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bbc4756-f831-46a5-b6ee-047dd13c6376\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bbc4756-f831-46a5-b6ee-047dd13c6376')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bbc4756-f831-46a5-b6ee-047dd13c6376 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         REF ALT     QUAL FILTER  \\\n",
              "2473683    C   T      338      0   \n",
              "5754588    A   G  225.417      0   \n",
              "5451621    T   C  123.371      0   \n",
              "6042902    C   T  203.417      0   \n",
              "6381692    A   G  34.9916      0   \n",
              "10105960   G   C    90.64      0   \n",
              "9729004    G   A   190.96      0   \n",
              "5228665    C   T   166.97      0   \n",
              "6544227    A   G  189.416      0   \n",
              "5186841    C   T  180.416      0   \n",
              "\n",
              "                                                       INFO  \\\n",
              "2473683   FR=1.0;MMLQ=37.0;TCR=7;HP=2;WE=99426289;Source...   \n",
              "5754588   DP=13;VDB=0.0431806;FS=0.0;SGB=-0.683931;MQ0F=...   \n",
              "5451621   DP=15;VDB=0.306246;RPBZ=-1.56372;MQBZ=-1.06904...   \n",
              "6042902   DP=9;VDB=0.293521;MQSBZ=-1.68732;FS=0.0;SGB=-0...   \n",
              "6381692   DP=8;VDB=0.71601;RPBZ=0.149071;MQBZ=-0.58554;B...   \n",
              "10105960  AC=1;AF=0.5;AN=2;BaseQRankSum=0.385;DP=10;Exce...   \n",
              "9729004   AC=2;AF=1.0;AN=2;DP=5;ExcessHet=0.0;FS=0.0;MLE...   \n",
              "5228665   DP=12;VDB=0.226073;RPBZ=-1.13389;BQBZ=0.755929...   \n",
              "6544227   DP=8;VDB=0.653727;FS=0.0;SGB=-0.636426;MQ0F=0....   \n",
              "5186841   DP=7;VDB=0.75529;FS=0.0;SGB=-0.636426;MQ0F=0.0...   \n",
              "\n",
              "                      FORMAT                            HG003  \n",
              "2473683   GT:GL:GOF:GQ:NR:NV  1/1:-37.3,-2.71,0.0:6.0:27:10:9  \n",
              "5754588                GT:PL                     1/1:255,39,0  \n",
              "5451621                GT:PL                    0/1:156,0,173  \n",
              "6042902                GT:PL                     1/1:233,27,0  \n",
              "6381692                GT:PL                     0/1:68,0,134  \n",
              "10105960      GT:AD:DP:GQ:PL           0/1:7,3:10:98:98,0,268  \n",
              "9729004       GT:AD:DP:GQ:PL            1/1:0,5:5:15:205,15,0  \n",
              "5228665                GT:PL                    0/1:200,0,104  \n",
              "6544227                GT:PL                     1/1:219,21,0  \n",
              "5186841                GT:PL                     1/1:210,21,0  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = dat.sample(1000000, random_state=42)\n",
        "print(df.groupby('FILTER').size())\n",
        "# Report the number of sentences.\n",
        "print('Number of training lines: {:,}\\n'.format(df.shape[0]))\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XmnnPRXJttH",
        "outputId": "419ef1be-cd51-460e-cede-f1fc07f56945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        FILTER                                      merged_column\n",
            "4581911      0  G A 96.2955 DP=14;VDB=0.0243922;RPBZ=1.80739;B...\n",
            "6260064      1  T G 3.83885 DP=7;VDB=0.607612;RPBZ=-0.353553;M...\n",
            "4743370      0  T C 225.417 DP=23;VDB=0.437369;FS=0.0;SGB=-0.6...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming you have a DataFrame called 'df' with columns 'col1', 'col2', 'col3', and 'col_to_exclude'\n",
        "\n",
        "# Get all column names except the one to exclude\n",
        "columns_to_merge = [col for col in df.columns if col != 'FILTER']\n",
        "\n",
        "# Define a lambda function to merge the values of selected columns\n",
        "merge_columns = lambda row: ' '.join(str(row[col]) for col in columns_to_merge)\n",
        "\n",
        "# Merge columns using the defined function\n",
        "df['merged_column'] = df.apply(merge_columns, axis=1)\n",
        "\n",
        "# Drop the original columns\n",
        "df = df.drop(columns=columns_to_merge)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df.sample(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3wGS_rE67m",
        "outputId": "cae97848-2eec-42a6-ba4c-2ff03cb2e73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWd_GQwoE_J7",
        "outputId": "15772ad6-6a86-4efd-ac77-c513649afff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyrcf3qnEzhE",
        "outputId": "c51dc2b8-f4c5-48d6-d86b-117f3d9c1030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T7mp_2zs2qB"
      },
      "outputs": [],
      "source": [
        "!tail -45 unique_words.txt > ww.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T1RhGWW01X8",
        "outputId": "043d056b-fbf8-4ef3-c816-7debe66bca19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['platypus',\n",
              " 'indel',\n",
              " 'idv',\n",
              " 'imf',\n",
              " 'dp',\n",
              " 'vdb',\n",
              " 'rpbz',\n",
              " 'mqbz',\n",
              " 'bqbz',\n",
              " 'mqsbz',\n",
              " 'scbz',\n",
              " 'fs',\n",
              " 'sgb',\n",
              " 'mq0f',\n",
              " 'dp4',\n",
              " 'mq',\n",
              " 'mmlq',\n",
              " 'tcr',\n",
              " 'ws',\n",
              " 'nf',\n",
              " 'tcf',\n",
              " 'mgof',\n",
              " 'sbpval',\n",
              " 'readposranksum',\n",
              " 'qd',\n",
              " 'brf',\n",
              " 'hapscore',\n",
              " 'gq',\n",
              " 'gof',\n",
              " 'gl',\n",
              " 'nv',\n",
              " 'badreads',\n",
              " 'allelebias',\n",
              " 'hp10',\n",
              " 'q20',\n",
              " 'strandbias',\n",
              " 'qualdepth',\n",
              " 'refcall',\n",
              " 'baseqranksum',\n",
              " 'excesshet',\n",
              " 'inbreedingcoeff',\n",
              " 'mleac',\n",
              " 'mleaf',\n",
              " 'mqranksum',\n",
              " 'sor',\n",
              " 'lowqual']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ww = open('ww.txt')\n",
        "never_split = ww.readlines()\n",
        "res = ['platypus']\n",
        "for sub in never_split:\n",
        "    res.append(sub.replace(\"\\n\", \"\"))\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUckAVYL3yEf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2klGRY62hKRi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel, BertTokenizer,  BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer(\"unique_words.txt\", do_lower_case = True,\n",
        "                          do_basic_tokenize = True, never_split = res,\n",
        "                          unk_token = '[UNK]', sep_token = '[SEP]', pad_token = '[PAD]',\n",
        "                          cls_token = '[CLS]', mask_token = '[MASK]',\n",
        "                          tokenize_chinese_chars = False, strip_accents = None)\n",
        "config = BertConfig(vocab_size = len(tokenizer), hidden_size = 768,\n",
        "                           num_hidden_layers = 6, num_attention_heads = 6,\n",
        "                           intermediate_size = 3072, hidden_act = 'gelu',\n",
        "                           hidden_dropout_prob = 0.2, attention_probs_dropout_prob = 0.2,\n",
        "                           max_position_embeddings = 512,\n",
        "                           initializer_range = 0.02, layer_norm_eps = 1e-12,\n",
        "                           pad_token_id = 0, position_embedding_type = 'absolute',\n",
        "                           use_cache = False, classifier_dropout = 0.5, num_labels=2,\n",
        "                           output_attentions = False, # Whether the model returns attentions weights.\n",
        "                           output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "model =  BertForSequenceClassification(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQfTYjVsOa98"
      },
      "outputs": [],
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.merged_column.values\n",
        "labels = df.FILTER.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR9LrVckQNJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = labels.astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8hgclWhOkDI",
        "outputId": "fd0c9678-0777-40dd-e540-56aae045a55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  C A 188 FR=1.0;MMLQ=35.0;TCR=6;HP=3;WE=34402384;Source=Platypus;WS=34402366;PP=188.0;TR=6;NF=2;TCF=3;NR=4;TC=9;MGOF=4;SbPval=0.65;MQ=60.0;QD=37.1285354245;SC=AACAGGTAAACCTCTAGCAAG;BRF=0.44;HapScore=2 GT:GL:GOF:GQ:NR:NV 1/1:-22.3,-1.51,0.0:4.0:15:9:6\n",
            "Tokenized:  ['c', 'a', '188', 'fr', '=', '1', '.', '0', ';', 'mmlq', '=', '35', '.', '0', ';', 'tcr', '=', '6', ';', 'hp', '=', '3', ';', 'we', '=', '344', '##0', '##23', '##8', '##4', ';', 'source', '=', 'platypus', ';', 'ws', '=', '344', '##0', '##23', '##66', ';', 'pp', '=', '188', '.', '0', ';', 'tr', '=', '6', ';', 'nf', '=', '2', ';', 'tcf', '=', '3', ';', 'nr', '=', '4', ';', 'tc', '=', '9', ';', 'mgof', '=', '4', ';', 'sbpval', '=', '0', '.', '65', ';', 'mq', '=', '60', '.', '0', ';', 'qd', '=', '37', '.', '128', '##53', '##54', '##24', '##5', ';', 'sc', '=', 'aa', '##ca', '##gg', '##ta', '##aa', '##cc', '##tc', '##tag', '##ca', '##ag', ';', 'brf', '=', '0', '.', '44', ';', 'hapscore', '=', '2', 'gt', ':', 'gl', ':', 'gof', ':', 'gq', ':', 'nr', ':', 'nv', '1', '/', '1', ':', '-', '22', '.', '3', ',', '-', '1', '.', '51', ',', '0', '.', '0', ':', '4', '.', '0', ':', '15', ':', '9', ':', '6']\n",
            "Token IDs:  [1039, 1037, 19121, 10424, 1027, 1015, 1012, 1014, 1025, 30537, 1027, 3486, 1012, 1014, 1025, 30538, 1027, 1020, 1025, 6522, 1027, 1017, 1025, 2057, 1027, 29386, 2692, 21926, 2620, 2549, 1025, 3120, 1027, 100, 1025, 30539, 1027, 29386, 2692, 21926, 28756, 1025, 4903, 1027, 19121, 1012, 1014, 1025, 19817, 1027, 1020, 1025, 30540, 1027, 1016, 1025, 30541, 1027, 1017, 1025, 17212, 1027, 1018, 1025, 22975, 1027, 1023, 1025, 30542, 1027, 1018, 1025, 30543, 1027, 1014, 1012, 3515, 1025, 30536, 1027, 3438, 1012, 1014, 1025, 30545, 1027, 4261, 1012, 11899, 22275, 27009, 18827, 2629, 1025, 8040, 1027, 9779, 3540, 13871, 2696, 11057, 9468, 13535, 15900, 3540, 8490, 1025, 30546, 1027, 1014, 1012, 4008, 1025, 30547, 1027, 1016, 14181, 1024, 30550, 1024, 30549, 1024, 30548, 1024, 17212, 1024, 30551, 1015, 1013, 1015, 1024, 1011, 2570, 1012, 1017, 1010, 1011, 1015, 1012, 4868, 1010, 1014, 1012, 1014, 1024, 1018, 1012, 1014, 1024, 2321, 1024, 1023, 1024, 1020]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebphJP8vbQee",
        "outputId": "33712f3d-38cd-45aa-8ece-e4618b0465dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  C A 188 FR=1.0;MMLQ=35.0;TCR=6;HP=3;WE=34402384;Source=Platypus;WS=34402366;PP=188.0;TR=6;NF=2;TCF=3;NR=4;TC=9;MGOF=4;SbPval=0.65;MQ=60.0;QD=37.1285354245;SC=AACAGGTAAACCTCTAGCAAG;BRF=0.44;HapScore=2 GT:GL:GOF:GQ:NR:NV 1/1:-22.3,-1.51,0.0:4.0:15:9:6\n",
            "Token IDs: [101, 1039, 1037, 19121, 10424, 1027, 1015, 1012, 1014, 1025, 30537, 1027, 3486, 1012, 1014, 1025, 30538, 1027, 1020, 1025, 6522, 1027, 1017, 1025, 2057, 1027, 29386, 2692, 21926, 2620, 2549, 1025, 3120, 1027, 100, 1025, 30539, 1027, 29386, 2692, 21926, 28756, 1025, 4903, 1027, 19121, 1012, 1014, 1025, 19817, 1027, 1020, 1025, 30540, 1027, 1016, 1025, 30541, 1027, 1017, 1025, 17212, 1027, 1018, 1025, 22975, 1027, 1023, 1025, 30542, 1027, 1018, 1025, 30543, 1027, 1014, 1012, 3515, 1025, 30536, 1027, 3438, 1012, 1014, 1025, 30545, 1027, 4261, 1012, 11899, 22275, 27009, 18827, 2629, 1025, 8040, 1027, 9779, 3540, 13871, 2696, 11057, 9468, 13535, 15900, 3540, 8490, 1025, 30546, 1027, 1014, 1012, 4008, 1025, 30547, 1027, 1016, 14181, 1024, 30550, 1024, 30549, 1024, 30548, 1024, 17212, 1024, 30551, 1015, 1013, 1015, 1024, 1011, 2570, 1012, 1017, 1010, 1011, 1015, 1012, 4868, 1010, 1014, 1012, 1014, 1024, 1018, 1012, 1014, 1024, 2321, 1024, 1023, 1024, 1020, 102]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEZV3QpIhWYC",
        "outputId": "e3d923c6-2a63-47af-89b0-8314bde2c991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  183\n"
          ]
        }
      ],
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsahMGf5w6u1"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPSvdKHFwr4c",
        "outputId": "52d10db0-c920-4117-adb8-b96f10349025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 190 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 190\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQC1ty7rw54x"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rpZEGXpwyL3"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 80% for training and 20% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=42, test_size=0.4)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=42, test_size=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76l9HQhm6s71"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YTT7Ce16s-X"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 500\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LTJ6YC6tAw",
        "outputId": "bcbb880d-e067-47cc-bbd4-d019bc9feed6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30567, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "#model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNuTrzjd6tDL",
        "outputId": "37749997-e8e5-4ec7-8e27-1cc03af59cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BERT model has 105 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30567, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCoEms-O6tFB"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW, Adam\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv4ck8Kx68Iu"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 30\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOOUWZC368Lh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqEyOq7_68NX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6yjqF1g7V_2",
        "outputId": "224b1f32-3269-42aa-c65f-a763e26913ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:03.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:06.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:08.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:10.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:12.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:14.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:25.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:36.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:47.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.36595\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[335428   9905]\n",
            " [ 36232  18435]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94    345333\n",
            "           1       0.65      0.34      0.44     54667\n",
            "\n",
            "    accuracy                           0.88    400000\n",
            "   macro avg       0.78      0.65      0.69    400000\n",
            "weighted avg       0.87      0.88      0.87    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.8846575\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8680702362983878, 0.8846575, 0.8684837482812269, None)\n",
            "  Accuracy: 0.88466\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.29609\n",
            "  Training epcoh took: 0:31:08\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339914   5419]\n",
            " [ 35157  19510]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.94    345333\n",
            "           1       0.78      0.36      0.49     54667\n",
            "\n",
            "    accuracy                           0.90    400000\n",
            "   macro avg       0.84      0.67      0.72    400000\n",
            "weighted avg       0.89      0.90      0.88    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.89856\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8893677538273469, 0.89856, 0.88170411467293, None)\n",
            "  Accuracy: 0.89856\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:38.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:49.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.27631\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338491   6842]\n",
            " [ 31483  23184]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.95    345333\n",
            "           1       0.77      0.42      0.55     54667\n",
            "\n",
            "    accuracy                           0.90    400000\n",
            "   macro avg       0.84      0.70      0.75    400000\n",
            "weighted avg       0.90      0.90      0.89    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9041875\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8953922639754056, 0.9041875, 0.891899700353197, None)\n",
            "  Accuracy: 0.90419\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:16.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:27.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:36.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:44.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:46.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:55.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:57.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:04.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:06.\n",
            "\n",
            "  Average training loss: 0.26488\n",
            "  Training epcoh took: 0:31:08\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[341933   3400]\n",
            " [ 34624  20043]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95    345333\n",
            "           1       0.85      0.37      0.51     54667\n",
            "\n",
            "    accuracy                           0.90    400000\n",
            "   macro avg       0.88      0.68      0.73    400000\n",
            "weighted avg       0.90      0.90      0.89    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.90494\n",
            "Weighted precision_recall_f1score:\n",
            "(0.900796283451761, 0.90494, 0.8879959911528275, None)\n",
            "  Accuracy: 0.90494\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:16.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:27.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:38.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:47.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:49.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.25799\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[342715   2618]\n",
            " [ 35615  19052]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95    345333\n",
            "           1       0.88      0.35      0.50     54667\n",
            "\n",
            "    accuracy                           0.90    400000\n",
            "   macro avg       0.89      0.67      0.72    400000\n",
            "weighted avg       0.90      0.90      0.89    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9044175\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9022170299304613, 0.9044175, 0.8859386712457208, None)\n",
            "  Accuracy: 0.90442\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:16.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:27.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:38.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:46.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:57.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:06.\n",
            "\n",
            "  Average training loss: 0.25295\n",
            "  Training epcoh took: 0:31:08\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[341295   4038]\n",
            " [ 32625  22042]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.99      0.95    345333\n",
            "           1       0.85      0.40      0.55     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.88      0.70      0.75    400000\n",
            "weighted avg       0.90      0.91      0.89    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9083425\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9035127260179254, 0.9083425, 0.8939391505276496, None)\n",
            "  Accuracy: 0.90834\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:50.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:27:59.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:01.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.24986\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339536   5797]\n",
            " [ 30160  24507]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.81      0.45      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9101075\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9034250740366356, 0.9101075, 0.8987518924987254, None)\n",
            "  Accuracy: 0.91011\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:16.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:27.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:38.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:57.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.24617\n",
            "  Training epcoh took: 0:31:08\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338031   7302]\n",
            " [ 28651  26016]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.48      0.59     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.73      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9101175\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9025906436911545, 0.9101175, 0.9005603066480061, None)\n",
            "  Accuracy: 0.91012\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:28.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:50.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:27:59.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:01.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.24321\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339493   5840]\n",
            " [ 29873  24794]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.81      0.45      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9107175\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9041227587205263, 0.9107175, 0.8996412590471239, None)\n",
            "  Accuracy: 0.91072\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:50.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.24025\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339373   5960]\n",
            " [ 29638  25029]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.81      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.911005\n",
            "Weighted precision_recall_f1score:\n",
            "(0.904374631955558, 0.911005, 0.900179434105803, None)\n",
            "  Accuracy: 0.91101\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:28.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:50.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:27:59.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:01.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.23804\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339967   5366]\n",
            " [ 30144  24523]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.82      0.45      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.87      0.72      0.77    400000\n",
            "weighted avg       0.91      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.911225\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9051491188154699, 0.911225, 0.8997551133823438, None)\n",
            "  Accuracy: 0.91123\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.23505\n",
            "  Training epcoh took: 0:31:08\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[336964   8369]\n",
            " [ 27465  27202]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.76      0.50      0.60     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.84      0.74      0.78    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.910415\n",
            "Weighted precision_recall_f1score:\n",
            "(0.902780827246573, 0.910415, 0.9021412262910319, None)\n",
            "  Accuracy: 0.91041\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:27:59.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.23266\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[340909   4424]\n",
            " [ 31299  23368]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95    345333\n",
            "           1       0.84      0.43      0.57     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.88      0.71      0.76    400000\n",
            "weighted avg       0.91      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9106925\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9056472280383017, 0.9106925, 0.897811445193365, None)\n",
            "  Accuracy: 0.91069\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:06.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:08.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:10.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:12.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:15.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.23037\n",
            "  Training epcoh took: 0:31:09\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338015   7318]\n",
            " [ 28239  26428]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.48      0.60     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.73      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9111075\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9037980634139235, 0.9111075, 0.9018970095376466, None)\n",
            "  Accuracy: 0.91111\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:55.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:57.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:27:59.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.22805\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339337   5996]\n",
            " [ 29712  24955]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.81      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.91073\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9040174273919835, 0.91073, 0.8998479913456678, None)\n",
            "  Accuracy: 0.91073\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:10.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:12.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:15.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.22523\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[340282   5051]\n",
            " [ 30684  23983]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95    345333\n",
            "           1       0.83      0.44      0.57     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.87      0.71      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9106625\n",
            "Weighted precision_recall_f1score:\n",
            "(0.904814672540365, 0.9106625, 0.8985814116609181, None)\n",
            "  Accuracy: 0.91066\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:06.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:08.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:10.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:12.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:14.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:25.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:27.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:36.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:38.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:47.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:49.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.22284\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338880   6453]\n",
            " [ 29392  25275]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.80      0.46      0.59     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9103875\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9033008143490349, 0.9103875, 0.899931163289181, None)\n",
            "  Accuracy: 0.91039\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.22036\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338588   6745]\n",
            " [ 29330  25337]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.79      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9098125\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9024428082318415, 0.9098125, 0.8995002830063777, None)\n",
            "  Accuracy: 0.90981\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:12.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:15.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:47.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.21789\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337955   7378]\n",
            " [ 28739  25928]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.47      0.59     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.73      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9097075\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9020630176111077, 0.9097075, 0.9000997970126974, None)\n",
            "  Accuracy: 0.90971\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:16.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:24.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:35.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.21537\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338062   7271]\n",
            " [ 28720  25947]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.47      0.59     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.73      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9100225\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9024839965357572, 0.9100225, 0.9003977219894593, None)\n",
            "  Accuracy: 0.91002\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:17.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:19.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:21.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.21283\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337992   7341]\n",
            " [ 28703  25964]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.47      0.59     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.73      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.90989\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9022988800987133, 0.90989, 0.9003011410569446, None)\n",
            "  Accuracy: 0.90989\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.21040\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[339472   5861]\n",
            " [ 30150  24517]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.81      0.45      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.86      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9099725\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9032100867388131, 0.9099725, 0.8986457363284004, None)\n",
            "  Accuracy: 0.90997\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:23.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:26.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:28.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.20826\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338425   6908]\n",
            " [ 29573  25094]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9087975\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9011197505796726, 0.9087975, 0.8983211271198809, None)\n",
            "  Accuracy: 0.90880\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.20627\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338020   7313]\n",
            " [ 29201  25466]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.47      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.908715\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9008582322075875, 0.908715, 0.8986925746475092, None)\n",
            "  Accuracy: 0.90871\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:30.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:32.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:34.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.20427\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338839   6494]\n",
            " [ 30132  24535]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.79      0.45      0.57     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.908435\n",
            "Weighted precision_recall_f1score:\n",
            "(0.9008931110947593, 0.908435, 0.8973216113349843, None)\n",
            "  Accuracy: 0.90843\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.20268\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[338259   7074]\n",
            " [ 29640  25027]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.78      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.85      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.908215\n",
            "Weighted precision_recall_f1score:\n",
            "(0.900328105330723, 0.908215, 0.8977316034878932, None)\n",
            "  Accuracy: 0.90821\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:37.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:39.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:41.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:48.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:59.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:01.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.20008\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337060   8273]\n",
            " [ 28812  25855]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.76      0.47      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.84      0.72      0.77    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.9072875\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8988838617824081, 0.9072875, 0.8979036546855619, None)\n",
            "  Accuracy: 0.90729\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:18.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:29.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:31.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:40.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:42.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:44.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:46.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:50.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:52.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:54.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:03.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:05.\n",
            "\n",
            "  Average training loss: 0.19896\n",
            "  Training epcoh took: 0:31:07\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337302   8031]\n",
            " [ 29097  25570]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.76      0.47      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.84      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.90718\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8987748072801205, 0.90718, 0.897477464263622, None)\n",
            "  Accuracy: 0.90718\n",
            "  Validation took: 0:06:53\n",
            "\n",
            "======== Epoch 29 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:17.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:19.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:28.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:30.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:32.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:43.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:45.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:47.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:49.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:51.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:53.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.19741\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337475   7858]\n",
            " [ 29188  25479]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.76      0.47      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.84      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.907385\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8990604534979691, 0.907385, 0.8975484852734802, None)\n",
            "  Accuracy: 0.90739\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "======== Epoch 30 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,200.    Elapsed: 0:01:02.\n",
            "  Batch    80  of  1,200.    Elapsed: 0:02:04.\n",
            "  Batch   120  of  1,200.    Elapsed: 0:03:07.\n",
            "  Batch   160  of  1,200.    Elapsed: 0:04:09.\n",
            "  Batch   200  of  1,200.    Elapsed: 0:05:11.\n",
            "  Batch   240  of  1,200.    Elapsed: 0:06:13.\n",
            "  Batch   280  of  1,200.    Elapsed: 0:07:15.\n",
            "  Batch   320  of  1,200.    Elapsed: 0:08:17.\n",
            "  Batch   360  of  1,200.    Elapsed: 0:09:20.\n",
            "  Batch   400  of  1,200.    Elapsed: 0:10:22.\n",
            "  Batch   440  of  1,200.    Elapsed: 0:11:24.\n",
            "  Batch   480  of  1,200.    Elapsed: 0:12:26.\n",
            "  Batch   520  of  1,200.    Elapsed: 0:13:28.\n",
            "  Batch   560  of  1,200.    Elapsed: 0:14:30.\n",
            "  Batch   600  of  1,200.    Elapsed: 0:15:33.\n",
            "  Batch   640  of  1,200.    Elapsed: 0:16:35.\n",
            "  Batch   680  of  1,200.    Elapsed: 0:17:37.\n",
            "  Batch   720  of  1,200.    Elapsed: 0:18:39.\n",
            "  Batch   760  of  1,200.    Elapsed: 0:19:41.\n",
            "  Batch   800  of  1,200.    Elapsed: 0:20:43.\n",
            "  Batch   840  of  1,200.    Elapsed: 0:21:45.\n",
            "  Batch   880  of  1,200.    Elapsed: 0:22:48.\n",
            "  Batch   920  of  1,200.    Elapsed: 0:23:50.\n",
            "  Batch   960  of  1,200.    Elapsed: 0:24:52.\n",
            "  Batch 1,000  of  1,200.    Elapsed: 0:25:56.\n",
            "  Batch 1,040  of  1,200.    Elapsed: 0:26:58.\n",
            "  Batch 1,080  of  1,200.    Elapsed: 0:28:00.\n",
            "  Batch 1,120  of  1,200.    Elapsed: 0:29:02.\n",
            "  Batch 1,160  of  1,200.    Elapsed: 0:30:04.\n",
            "\n",
            "  Average training loss: 0.19624\n",
            "  Training epcoh took: 0:31:06\n",
            "\n",
            "Running Validation...\n",
            "Confusion Matrix:\n",
            "[[337649   7684]\n",
            " [ 29446  25221]]\n",
            "Classification reporet:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95    345333\n",
            "           1       0.77      0.46      0.58     54667\n",
            "\n",
            "    accuracy                           0.91    400000\n",
            "   macro avg       0.84      0.72      0.76    400000\n",
            "weighted avg       0.90      0.91      0.90    400000\n",
            "\n",
            "Accuracy score:\n",
            "0.907175\n",
            "Weighted precision_recall_f1score:\n",
            "(0.8988343232619722, 0.907175, 0.8970590315935904, None)\n",
            "  Accuracy: 0.90718\n",
            "  Validation took: 0:06:51\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "val_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Initialize variables for the confusion matrix\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # Append predictions and labels to calculate the confusion matrix\n",
        "        all_preds.extend(np.argmax(logits, axis=1))\n",
        "        all_labels.extend(label_ids)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    confusion = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion)\n",
        "    print(\"Classification reporet:\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "    print(\"Accuracy score:\")\n",
        "    print(accuracy_score(all_labels, all_preds))\n",
        "    print(\"Weighted precision_recall_f1score:\")\n",
        "    print(precision_recall_fscore_support(all_labels, all_preds, average = 'weighted'))\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    val_values.append(eval_accuracy/nb_eval_steps)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A5-Mz8x87WCb",
        "outputId": "715eff3f-539b-4134-8292-66979ba47992"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABAYAAAI/CAYAAADk/9uGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACFvElEQVR4nOzdd3zV1eH/8fed2SEJIxCGIGICaGQ4oFKtiBonaC24aqlVWnC0pf6qtoVvndjWahUVJ6BULFhFHBUoKhbBACIQ2SushB0yyLq5935+f4R7ySXrJvlkXO7r+SjNvfdz7uecT3K8yX3fMyyGYRgCAAAAAABhydraDQAAAAAAAK2HYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAACAMLdixQqlpqYqNTXV9HN/8MEHSk1N1fDhw00/d3N7+OGHlZqaqocffri1mwIAQLOyt3YDAAAIB0150z1lyhTddNNNJrYGAADgJIIBAABaQIcOHWp8vKSkRCUlJXWWiYyMbLZ2SVJUVJR69erVLOeOi4tTr169lJyc3CznBwAATUcwAABAC1i2bFmNj0+dOlUvvvhinWWaW3p6uhYsWNAs577iiit0xRVXNMu5AQCAOVhjAAAAAACAMMaIAQAA2jDf2gRvv/22zjrrLL322mtasmSJDhw4oLKyMm3ZskWSVFpaqs8//1z/+9//tGXLFh08eFDHjx9XQkKC0tPTNWbMGF166aU11rFixQrdeeedkuQ/n88HH3ygRx55RF27dtUXX3yh9evX6/XXX9fq1auVn5+v5ORkjRgxQhMmTFC7du2qnfvU51flGy1x4YUXatasWfrmm280Y8YMZWVlqbi4WN26ddO1116re+65RxEREbV+jxYvXqy3335bGzdulMfjUffu3XX99ddr7NixeuWVVwLqMNuKFSv0zjvvaM2aNTp27JhiYmKUlpamG264QaNGjZLNZqvxeevWrdPbb7+tNWvW6PDhw7LZbEpMTFTXrl01dOhQ/fjHP1bnzp0DnrNjxw7NnDlTK1eu1IEDB+T1epWUlKTk5GQNGTJEI0eOVO/evU2/RgDA6Y9gAACAELBnzx5NnDhRR44cUUREhOz2wF/hn332mR555BFJksViUWxsrOx2uw4fPqzPP/9cn3/+ue666y499NBDjW7Dxx9/rEceeUQVFRWKi4uTx+PRvn37NHPmTC1btkxz5sxRTExMo879xhtv6JlnnpFUuS5BRUWFdu7cqalTp2rlypWaMWNGjW+y//KXv2j69On++/Hx8dqxY4eeeeYZffXVVxo8eHDjLjYIU6ZM0cyZMyVVfs/j4uJUVFSkzMxMZWZm6qOPPtJLL72k2NjYgOfNmzdPjzzyiAzDkCQ5nU7ZbDbl5uYqNzdXq1atUpcuXQIWnFy2bJl+9atfyeVySZIcDoeioqJ04MABHThwQOvWrZPD4dD999/fbNcLADh9MZUAAIAQ8NRTTykuLk4zZ87U2rVr9d133wWsCxAfH6+77rpLs2fP1po1a/Ttt99q7dq1Wrp0qe6//345HA5Nnz5dn3/+eaPqz8vL0x/+8AeNGjVKS5Ys0bfffqvvvvtOkydPlsPh0LZt2/TGG2806tybN2/W3//+d40bN07Lly/XqlWr9O233+ree++VVPmp/Lx586o979NPP/WHAtddd53+97//adWqVfruu+/0+OOPKysrS++++26j2lSff/7zn/5QYMyYMVq6dKm/3Y888ojsdrsyMzM1adKkgOeVlpbq8ccfl2EYuuGGG/Tf//5X33//vVavXq01a9bo/fff1y9+8Qu1b98+4Hl//vOf5XK5NGzYMH388cdav369Vq1apaysLH3yySe6//771bVr12a5VgDA6Y8RAwAAhACr1aqZM2cGDC+vupPAiBEjNGLEiGrP69Spk+677z5FRUXpr3/9q2bNmqXLL7+8wfWXlpbqxhtv1BNPPOF/LCoqSrfffrv27t2rGTNm6NNPP9Wvf/3rBp+7sLBQ9913X8Cn3bGxsXrggQe0bds2LVq0SJ9++qluvvlm/3HDMPT8889Lki6++GI988wzslgskqSIiAiNHj1adrvdP4rCTGVlZZo6daqkykDiscce8x+Ljo7W2LFjZbPZ9MQTT+g///mPfvGLX+icc86RJG3btk3FxcWKjo7WlClTAkZ+REdH65xzzvGX9Tl69Kj27NkjqXKUQqdOnfzHIiIi1KdPH/Xp08f06wQAhA9GDAAAEAJGjhxZbc55Q/zoRz+SJK1du1Yej6dR5xg/fnyNj/uCht27d6u0tLTB53U6nbrrrrvqPPepax9s2rRJu3fvliT98pe/9IcCVd14441KSUlpcHvqs2zZMuXn50uS7rvvvhrL3HbbberYsaMk6ZNPPvE/HhcXJ0mqqKjwn6M+MTExslor/2Q7fPhwI1sNAEDtCAYAAAgBgwYNqrfMkSNH9MILL2jMmDG66KKL1K9fP6Wmpio1NVXXXHONpMpP/gsKChpcf0JCgs4444waj1X9BLuwsLDB5+7Tp0+taxP4zn1qmzds2CCpcq79wIEDa3yuxWLRBRdc0OD21Gf9+vWSpC5dugSM2qjKZrNpyJAhAeUlqUePHjrzzDNVUVGh0aNH67XXXtOmTZvqDGsiIyM1dOhQSdLdd9+t559/XuvWrfOvNwAAQFMRDAAAEAJOnXN+qjVr1ujqq6/WSy+9pLVr1yo/P18RERFq3769OnTooMTERH/ZxnyqX9eiglUXBayoqGiWc7vd7oDHjx07JqkysHA6nbU+Pzk5ucHtqc/Ro0eDOrdvhIevvFR5Pc8995y6deumnJwc/f3vf9eoUaM0ePBg/fznP9fs2bNr/Pk88cQTSktLU15enl5++WWNHj1agwYN0q233qo33ngj6NEHAADUhDUGAAAIAb6h5DVxu9363e9+p8LCQvXt21e//e1vNXjw4IDV8Pfs2aMrrrhCkvyr4aN1pKWl6bPPPtOSJUv09ddfa82aNdq2bZuWL1+u5cuX67XXXtOrr77q36pSklJSUjRv3jwtW7ZMX331lb777jtt2bJF3333nb777ju99tprev755/0jCwAAaAiCAQAAQtzatWuVk5Mjm82mV199tcZPsk+3uem+ERD5+flyuVy1jho4ePCg6XX7Rm8cOHCgznK+4zWN9nA6nbryyit15ZVXSqocAbFw4UI999xz2r9/vx5++OFqOzFYrVb98Ic/1A9/+ENJ0vHjx/Xll1/q2WefVW5urh588EF9+eWXdY6gAACgJkwlAAAgxO3fv1+SlJSUVOvw9m+++aYlm9Ts+vfvL6ly6sKaNWtqLGMYhr799lvT6/btGnDgwAFlZ2fXWMbj8WjFihWSpHPPPbfecyYmJuqWW27Rgw8+KEnauHGjf7pEbWJjY3X99dfrySeflFS5xsTWrVuDvg4AAHwIBgAACHG+le6PHDmiI0eOVDt+4MABzZo1q6Wb1az69u3rXwzxtddeq3F6xPz585WTk2N63RdffLESEhIkSS+++GKNZf71r3/p0KFDkqRrr73W/3h9CwZGRET4b/umjzTmOQAANAS/PQAACHGDBw9WdHS0DMPQb37zG/+n2B6PR0uXLtVPf/rTVm6h+SwWi+6//35J0tdff62HHnrIP22gvLxc7733nv7v//5P7dq1M73uyMhIf92ffPKJJk+e7A9kSktL9fbbb2vKlCmSpGuuucY/wkCSPv30U91yyy3617/+pb179/of9/2s/v73v0uSBg4c6G/7mjVrdP3112vmzJnasWOHvF6vpMoREd99953+/Oc/S6pc7LDqugQAAASLNQYAAAhxcXFx+v3vf68///nPWrVqlTIyMhQdHS2Px6Py8nIlJiZqypQpGj9+fGs31VTXX3+9vv/+e7311luaP3++PvroI8XHx6ukpEQVFRUaMmSIzjvvPL366qumz7u/4447tHfvXs2cOVNz5szR3LlzFR8fr+LiYv8OChdddJEef/zxgOcZhqE1a9b4pz84nU5FR0ersLDQ/4a/U6dO/ukBPlu3btWUKVM0ZcoUORwOxcTE6Pjx4/66YmNj9fe//z1ghwgAAIJFMAAAwGng1ltvVUpKit544w2tX79eHo9HycnJuvTSS3XPPfc0ahvBUPCHP/xBF1xwgd5++21t3LhRLpdLZ555pkaOHKmf/exnevrppyVJ8fHxptf9yCOP6LLLLtPs2bP13XffKT8/XzExMUpLS9PIkSM1atSoam/Uhw8frr/85S9asWKFNm7cqMOHD6ugoEAxMTHq1auXLrvsMt1xxx0B7T333HP1j3/8QytWrFBWVpYOHTqk/Px8OZ1O9enTRxdffLHuvPPOZtmaEQAQHiwGexYBAIDT1C233KI1a9bogQce0L333tvazQEAoE1ijQEAAHBaWrlypX/Ivm+LPwAAUB3BAAAACFmPPvqoPvjgAx0+fNi/M0FhYaH+9a9/acKECZKkIUOGKD09vTWbCQBAm8ZUAgAAELJGjhypzZs3S6pcyC8qKkqFhYX+kOCss87S9OnTmX8PAEAdCAYAAEDI+vzzz7V48WJlZWXpyJEjOn78uGJjY3XWWWfpiiuu0JgxYxQVFdXazQQAoE0jGAAAAAAAIIyxxgAAAAAAAGGMYAAAAAAAgDBmb+0GhAPDMOT1hsaMDavVEjJtRdtEH4IZ6EcwA/0IZqAfoanoQzBDQ/uR1WqRxWIJujzBQAvweg3l5RW3djPqZbdblZgYo8LCErnd3tZuDkIQfQhmoB/BDPQjmIF+hKaiD8EMjelHSUkxstmCDwaYSgAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCmL21GwAAaJu8XkNb9+Yrv7hcCTEROrt7gqxWS7PXuWlXniqyj8lhMdQ7pV2z1xlOWuNnCgAA2j6CAQAhq7Xe5LTWG+aWrHP1lkOavXibjhWV+x9LjIvQbSP6aHBqp9OmTik8fp5S+H1/WzpgCpd+FC51+uoNh34EAJJkMQzDaO1GnO48Hq/y8opbuxn1stutSkyM0bFjxXK7va3dHIQYr9fQjtwCVRiWFvkDqrXe5ITDG+bVWw7ppXnraz1+743nmF5va9Tpq/d0/3n66uT7S53U2bbrDbfwLlzqbMm/jXD6asz7tKSkGNlswa8cQDDQAggGcLoLhzeurVVvS9fp9Rr6f9OWB/wsT5UUF6G/jv+BaX/ctEadUnj8PCW+v9RJnaFQL+EddZopnIKXcKhTIhioUWZmpmbMmKF169appKREKSkpysjI0Lhx4xQdHd3g8x07dkwzZszQ559/rn379slut+uss87STTfdpJ/85CeyWpu+PiPBQNsRLi8gLVlnW3zj2i7Gqd/fNtDUa/Z6Df119hoVFLtqLRMf7dC9N50riyzyGoa8XkMew5DhNU7cl/9xr1H5z6jy2MmylfW5vV59snyXSss9tdYZ6bTpsoFdJUmGJMMwZBiq/CffbePEscr/8xqVpb1G5ZNOHjeUX+zSpl3H6v1+9E6JV1y0U94Tv0J89Z34n1SlTiOgzCnHZKi0zK3coyX11nlmSrzio52y2SyyWS2yWi2yWU58tVpks1r9t60n/tmtlmqP2awWWSzS+1/tVEmZu9b6YiLtuvWKPrLIIhnyX6vXOHmdNX9vayhjSF7Dq0+/2V3vz3P4oK7+n6HXqPIzNCSvTvYRo8oxb5Wft9drVKnTUGGxSztyC+v9/vbp1k7xMU5ZLBZZLZLFUvl9suiU+6cet1hk0cn7VotFkqEla3NV5qr9WqOcNl1xQfcT5U+o+aZUpYyl5odlGIY+y9yj0jrqjI6w6/qLe57oA5VPtlY2/sS1nrwe+a9FNV6nz6yFW1RcRz+Ki3boVzf0l91ulfVEf7X6vlcnbld+lf+2pepjvjIWi6zWyn7xyGuZIR/eVe2jvvu+1ySvYcjjMTR5+grlH6/9NTch1qk///xCf52+n4ul6v9X7VMn+rOqlDt5vPJn4vUaeuTVTB07Xvu1JsZFaMq4IQHXevKvaaPaYwF/aPsfO/m66PUamvRm3ddKeEedDak7HEKQcKnTh2DgFLNmzdKTTz4pwzDUuXNnJSUlafv27XK5XOrdu7dmz56thISEoM+3c+dO3XXXXdq/f78cDof69Omj8vJy7dy5U4Zh6LLLLtOLL74ou71pSzEQDLQN4fIC0pJ1BvPHYly0Qz+/Jk1ut6EKt1cut0cut1euCs+J+15VVFQ+7rvvcnv8j516vMzlkccbMi9bANAqHDarrLaqb5FPeRNc7THf/SohzImbHo9XJXUEWj4RjspwrqbQ0R9YGoZ4BW+cSKdNEU6b7Far7DaL7DarbCe+2q0W2WzWytu+x2xVHrMGlrdZpc9W7KkzqIyJtOuWy/tUBmQBoYsC7lhqCGH8RSwn+6DXMOoN0mKjHLrr2jRZLdYTod0poZxOBnWS/G3zhT6WU257vYb+8V6WCktqD13axTj10O2DKgPDE409GQpWCQdP3Di1DSe/PZV1GoahP73R8kGPFD4hSLjUWRXBQBXr16/XT37yExmGoUcffVSjR4+WxWLRwYMHNX78eG3YsEFXXnmlpk6dGtT5PB6PRo4cqW3btmnw4MF6/vnn1bFjR0nSli1b9Ktf/Uq5ubmaMGGCfv3rXzep7QQDrS9cXkDMrNPt8aq4tEJFpRU6XuL76gq4f/BosXYdPG5W803lOPEHUTBq+mPmVG6PVxXu+l8u46Icioqwy3Li0z6b79NB68lP/E5++nfik9cTxwLLSnlFZdq+r/5Pes89M0ld2sfU+weSVPWPqMA/bnzlDueX6Ms1ufXWeeUF3ZXSIUaS/H9IVa2n6ies1Y+drFuyKOfIcX24NLveOjMu6q7kxOgToylOjLzwGvJU+eo59THDkNfrrXbsSEGZdh8oqrfOlA4xSoh1+v8oDfiDUVW+d1WvqcbvrXSkoEzb9hXUW+c5vZKU0iHG/6nyqZ/Un/p45f1aPtW3WnQwr0QLV+6tt94rL+iu5MSoOkcj+O+fetx3/0S53CPHlbUjr946+56RqOSkE6P9qvw5UvW/NKOmj1tV/dPYw8dKtWVvfr119k6JV/t2kSfevPraffIapBpGh9QwAsZrSIXF5TqQV1pvnfExTkU6bP6RQt4qI4MMo+YRRaHx1xmAxop0WOVw2Pwj3nyj4are9418s50Ifvyj5Wp4jtVi0dLv96u8rpFaETZdfVGPE78rqgSHp4Q80qkjcE4+ULW8YRj66OtdKimvPeyJjrDruh+cIYvFUuV12wgYiXfqiMKqI+4CjqvyNfKL73LqHJEW6bTp8sFdZbFYA0YG1fa7uuox398svku2SPJK+njZLpXWcZ3NFfb4tEQwEDK7Erz88svyer0aNWqUxowZ4388OTlZzz77rK6++motWrRImzdvVlpaWr3n++qrr7Rt2zY5nU799a9/9YcCkpSamqrJkyfrV7/6lWbMmKGf//znio+Pb5brQvPzeg3NXrytzjLvLt6mgX06Nvk/Zt8fdW63V+/8d2udZd/57zb1PSNRdpv1xNDRk3/gN6ruIK7zn4u2Ki7aqZIyt4pKXDpe9Y3/KffrepFvqA7xkUqIj5DTbpXTbpPDbq287Thx22GVw247cfzEbYf1RLnK277n7T1YpFc/3lhvnb8dPUBpZySadg2bdx/TX99dU2+58aPOMa3eYOu8+qIzTKvT6zW0dvvReocMj77sLNN++Q3s00Ffrc2tt86bLzWvzmC/t3dccXaL/zyvGWLez1Oq/Jmu3HSoRX+mm3cfCyoYuP4HPVv8+/vjS3u3eJ2/uqF/g+usOtTeqBIcbNlzTC+8/329z7/n+n7q3bWdP0GpOtLdd/vUz4YCwpYqz8veX6gZ/9lcb513X9dXvVPa1Tj1ompIVjVkC5jCUSVE3Lo3X8/OXVdvnQ/eMkCpPRIC2n/y9qlvKnzXb9Rafuve/KC+vw/8+Fyd3T1BquGNVFUn31xVOXjKG65gr/Wua/qqR3Ks3B5Dbo9XHo9Xbq/vduVXt6dyGtrJ+94q5U885jW0/0hxUEFatw4xahfrrNJnqpep6TPGmqZRBBukdWgXqZhIR2Agd8obSX+wp8ARKf5pcqos66rw1Dkqwsd+4k34yTekp74pPTk9zCxlFV6VVbTsB3Gl5R598L/6g3gzlZS7NffLHS1aZ5nLo0+/2dOideYVlWvr3nxTf3+3tJAIBoqLi7V06VJJ0ujRo6sd79mzp4YMGaLly5drwYIFQQUDq1evliSdc8456tatW7Xjl112maKjo1VSUqLPP/9cN954YxOvAqdq7nnwhmGopNyt1VsO1/kHsVT5H/OjM1cq0mn3f1pT7ZPHUx6v+pjvdkM+4ck/Xq77/rG02uO+P5B8nyz7Pj0O+FSwypxT3zzVCre33ussKHbp6Xe+C7qNFkkxUQ7FRTsUG1X5Ly7aobhop2KjHCoqcek/mfW/8N51bV/TXig7J0Vr7pId9b7JqfyDzTxnd09QYlxEi9bbGnVarRbdNqJPnSNPbh3Rx9T/VlujznD5eUp8f0O1Tv+IkMDB20rv3SGoOi/qm2zazzSlfYw+XJpdb51D+nU2rc5+PZOCus60Homm9t1gv7/pvTu0+LX+4Bzzvr/Bhlq3tUI4etc15v3NEGydExvwYYJ/OkyVUVKVj1c+uHlPvp4LIuj5xbV9dUbnOHk8vhFvleGN58QaG/77J/7mdPtGv3mq/m16IvDxGtp7qEjrth+tt97U7gnqkBB5otEBXwLWyjh12YyaAsWjBWVBrWFzVtd4dUiICphyUXXNj6qjCE+dtuE/fuLGwbwSrc+uP3ju3ytJnROjK39Gvmurui6QTo4YU5Xwx7cOU9WRC0fyS4O6zvziuv8Ob+tCIhjYtGmTXC6XnE6n0tPTaywzePBgLV++XOvW1f8foiQVFFQO50xOTq61TKdOnbRr1y6tWbOGYMBkZsyDL3d5lFdUprzCcuUVlimv6JSvheUqr6g/JfbZe6htTPfwpd9ej++eueKiHOqQEKnYKGeVN/q+N/7OgPsxkY46/wjxeg19s+Hgaf/GtbXqba1rHZzaSffeeE61/0aT4iJ0azOtj9HSdYbTz1Pi+0ud1NmW6yW8C606qw45P2XVBUlS/yCDnqH9zQt6pMoQJJhgYOSwXi0evNx0ibkjtYIJBq41cfRdsNeZEBNhSn2tJSSCgezsyiEvKSkpcjgcNZbp0aNHQNn6xMXFSZIOHjxYa5lDhw5JqlykEOapbR78saJyvTRvve698Ryl9+6gY743/f6vJ9/wHysqq3PxmqoiHdaghmpdf3FPde8YGzB/6+QK5pYaH6+6KnrVFc+37yvQ8//OqrfO3/7kPJ3VrZ1/nm7lKvUn5+xWnWvqG5ngv+17/ESZXQeK9G49Uwkkc4e6h9Mb19aqtzWvdWCfji26o4avzpba8zmcfp6+ulvyZxou31/qPL3qbK16Ce+os6lOl+CFOltPSAQDvk/327VrV2sZ3zFf2fqce+65kioXNczJyVHXrl0Djn/11VcqKSlp0DnrYrc3fdvD5uZbnKIhi1Q0lNdr1Pvm9eUP1wc9JD/SaVP7+EglxUcqKT4i8Ha7ytsOm1UTp36tvLr+Y46P0I8v7W3enOmzOyopLqLeOs/rY95QxNQeiVq4Yk+9dfbrlWTqL6OL+neW1WbVOwu3BNSdFB+h269M1QVpzfOH20X9O+uCvsnasueY8o+7lBDrVKrJQ0rbSr2tda2SdE7v9s1eR/U6Oyg+PkqFhaXyeJp3/mW4/Tyllv2Ztub3d1tOgcrdhiLsFvXp2nwBU9U6T/d+FC51Vq33dO5HrfH7mzqb92+jO65K1dQ6Ppi6/apUOZ026gyROqtqifdpIbErwUsvvaQXXnhB559/vt55550ay3zzzTcaO3asbDabNm6sf2Eyl8ulK664QgcOHNCAAQP0j3/8Q126dJEkZWVl6YEHHtD+/fslVY5G+O9//9vo9huG0egF5U4nHo9Xny3fpVc/rH9RH0ly2q3qkBDl/9exhtsxUTWPIDnV8qxcTXlrVa3HH/nZBfpBekpQ5wpWuNTp4/Ea2rjzqPIKy5QUH6l+Z7aXrYXe6AAAgMZpjd/f1Nl8lmfl6rUPv9fRgjL/Yx0SonTPyHOa7W9A6my+OltSSAQDb7zxhv72t7/pvPPO09y5c2ss89VXX2ncuHGKjo7WmjX1zwGRpDVr1uiee+5RUVGRbDabzjjjDJWXlysnJ0cJCQm68MILtWjRIvXt21cffvhho9vv8XhVWFj/KqytzWazmv4pXV5hmb7feVTf7ziq9dl5Kgly+P/Pr0nTjwZ2NTVQWbX5UIsnt+FSp09z9CGEH/oRzEA/ghnoR2iq1uhDXq/R4qNsqLN5NaYfxcdHnX7bFQYzTSCY6QanGjhwoObNm6fXX39dX3/9tfbu3at27drppptu0gMPPKBXXnlFktShQ4cmtL5SsPtNtgUej7fR7a1we7V1X7427MzT99lHlXM4cEG/YOf7d2wXJY/HkJmL7w08q4POO7N9jfNrm+vnEy51nqopfQjwoR/BDPQjmIF+hKZq6T7Up1uC/7ZvNy3qDN06fZqzH4VEMNCzZ09JUm5urioqKmpcgHDPnj0BZYPVvXt3PfbYYzUe2759u6ST6xGgOsMwdOhYqb7fWTkiYPOeY3JVeeNvkXRmSrz690rSuWe21xnJcXro1W9abfEOq9XS4vuLhkudAAAAAEJTSAQDffv2lcPhkMvlUlZWlgYPHlytzOrVqyVJAwYMMKXOvLw8rV27VpJ0+eWXm3LOtszrNbRpV54qso/VuxJ4ablbm/cc0/qdeVqffVSH88sCjreLdeqcE0FAv55Jij1lHYDW2q4LAAAAAFBdSAQDsbGxGjZsmL788kvNnTu3WjCwa9cuZWZmSpIyMjJMqfMf//iH3G63zj//fJ1zzjmmnLOtWr3lULXtcRLjInTbie1xDMPQ3kPHtT47T+t3HtW2fQXyVBkyY7Na1KdbO517Znudc2Z7desYU+faAK25XRcAAAAAIFBIBAOSNGHCBC1ZskTz58/XoEGDNHr0aFksFh06dEgTJ06U1+vViBEjlJaWFvC84cOHS5J+//vfVwsNvvrqK8XGxgYEDYWFhZo6darmzJmj6OjoWqcZnC5WbzlU46f3x4rK9dK89UrrnqD9eSUqKHYFHO+YEKlzzmyvc3u1V9oZCYp0NqwrtcYe6QAAAACA6kImGEhPT9fDDz+sp59+WpMnT9a0adOUmJio7du3y+VyqVevXnr88cerPS8nJ0eSVFJSUu3Y119/rbfffluxsbHq2rWrJGnnzp2qqKhQQkKCXnzxRfXu3bt5L6wVeb2GZi/eVmeZzXvzJUlOh1V9eyTqnDPb65wzk5ScGN3k+pkHDwAAAACtL2SCAUkaO3asUlNTNX36dGVlZeno0aNKSUlRRkaGxo0bp5iYmAadb8SIETp69Ki+//577dmzRxaLRb169dLw4cM1duxYJSae3m9at+7Nr3MRQJ/Rl/XW5YO7y2EPfrsLAAAAAEBoCKlgQJKGDh2qoUOHBl1+y5YttR676KKLdNFFF5nRrJCUX1x/KCBJCXERhAIAAAAAcJri3V4YS4iJMLUcAAAAACD0EAyEsbO7Jygxru43/UlxlYsCAgAAAABOTwQDYcxqtei2EX3qLHPriD7sFAAAAAAApzGCgTA3OLWT7r3xnGojB5LiInTvjedocGqnVmoZAAAAAKAlhNzigzDf4NROGtino3bkFqjCsMhhMdQ7pR0jBQAAAAAgDBAMQFLltIK+PZOUmBijY8eK5XZ7W7tJAAAAAIAWwFQCAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBmb+0GNFRmZqZmzJihdevWqaSkRCkpKcrIyNC4ceMUHR3d4PPl5uZq+vTp+vrrr7V//355vV517NhRF110kcaOHavU1NRmuAoAAAAAANqGkBoxMGvWLI0dO1ZLlixRRESEevfurZycHE2bNk0333yz8vPzG3S+NWvW6LrrrtOsWbO0b98+denSRT179tTRo0f1wQcf6KabbtJnn33WPBcDAAAAAEAbEDLBwPr16/XUU09Jkh577DEtWbJE8+bN0+LFi9W/f3/t2LFDkyZNCvp8hmHooYceUnFxsQYOHKhFixZpwYIF+vjjj/X111/ruuuuk9vt1p/+9CcVFRU112UBAAAAANCqQiYYePnll+X1ejVy5EiNGTNGFotFkpScnKxnn31WVqtVixYt0ubNm4M63/bt27V7925J0p///GelpKT4j8XFxWnKlCmKjo7W8ePH9e2335p/QQAAAAAAtAEhEQwUFxdr6dKlkqTRo0dXO96zZ08NGTJEkrRgwYKgzllWVua/3b1792rHnU6nkpOTJUlut7vBbQYAAAAAIBSERDCwadMmuVwuOZ1Opaen11hm8ODBkqR169YFdc5evXopMjJSUuVaA6c6dOiQ9u3bJ5vNpn79+jWy5QAAAAAAtG0hEQxkZ2dLklJSUuRwOGos06NHj4Cy9YmNjdWECRMkSY888ogWLFigY8eO6fjx48rMzNS4ceNUUVGhcePGqWvXriZcBQAAAAAAbU9IbFdYUFAgSWrXrl2tZXzHfGWD8ctf/lIdO3bUm2++qV//+tcBx3r27KnnnntO11xzTSNaXJ3d3vYzGJvNGvAVaCj6EMxAP4IZ6EcwA/0ITUUfghlaoh+FRDBQXl4uSbWOFpAq1wSoWjYYFRUV2rt3rwoKCmS329WtWzc5HA7t3r1bu3fv1r///W8NGjRInTt3blL7rVaLEhNjmnSOlhQfH9XaTUCIow/BDPQjmIF+BDPQj9BU9CGYoTn7UUgEAxEREZIq38jXxuVyBZQNxn333aclS5bokksu0RNPPOFfbLCgoEBPPPGEPvroI40ZM0affvqpYmNjG91+r9dQYWFJo5/fUmw2q+Ljo1RYWCqPx9vazUEIog/BDPQjmIF+BDPQj9BU9CGYoTH9KD4+qkEjDEIiGAhmmkAw0w2q+uKLL7RkyRIlJibq2WefVVxcXEB9Tz31lNavX6+dO3dq9uzZGjduXBOuQHK7Q+eFwOPxhlR70fbQh2AG+hHMQD+CGehHaCr6EMzQnP0oJCa79OzZU5KUm5tb66iBPXv2BJStz7fffitJSk9PDwgFfBwOhy666CJJ0vr16xvYYgAAAAAAQkNIBAN9+/aVw+GQy+VSVlZWjWVWr14tSRowYEBQ5ywuLg66/oasWwAAAAAAQCgJiWAgNjZWw4YNkyTNnTu32vFdu3YpMzNTkpSRkRHUOXv16iVJysrKUlFRUbXjFRUVWrFiRUBZAAAAAABONyERDEjShAkTZLFYNH/+fM2ZM0eGYUiSDh06pIkTJ8rr9WrEiBFKS0sLeN7w4cM1fPhwLViwIODxjIwMOZ1OHTt2TBMnTtTBgwf9xwoKCvSHP/xBO3fulMVi0Q033ND8FwgAAAAAQCsIicUHpcq1AB5++GE9/fTTmjx5sqZNm6bExERt375dLpdLvXr10uOPP17teTk5OZKkkpLAXQE6d+6sxx9/XH/84x/1v//9T8OHDw/YrtDlcslisejBBx9Uv379WuQaAQAAAABoaSETDEjS2LFjlZqaqunTpysrK0tHjx5VSkqKMjIyNG7cOMXExDTofKNGjVJaWpreeustffvtt8rNzZVhGOrYsaMGDhyo22+/XYMHD26mqwEAAAAAoPVZDN+YfDQbj8ervLzgFztsLXa7VYmJMTp2rJjtVNAo9CGYgX4EM9CPYAb6EZqKPgQzNKYfJSXFyGYLfuWAkFljAAAAAAAAmI9gAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMKYvbUb0FCZmZmaMWOG1q1bp5KSEqWkpCgjI0Pjxo1TdHR00OdZsWKF7rzzzqDK3n///brvvvsa22QAAAAAANqskAoGZs2apSeffFKGYahz587q0qWLtm/frmnTpmnRokWaPXu2EhISgjpXXFycBg0aVOvx48ePa+vWrZKkgQMHmtF8AAAAAADanJAJBtavX6+nnnpKkvTYY49p9OjRslgsOnjwoMaPH68NGzZo0qRJmjp1alDn69evn959991aj7/44ovaunWrunTpoqFDh5pyDQAAAAAAtDUhs8bAyy+/LK/Xq5EjR2rMmDGyWCySpOTkZD377LOyWq1atGiRNm/e3OS6DMPQhx9+KEkaOXKkrNaQ+TYBAAAAANAgIfGOt7i4WEuXLpUkjR49utrxnj17asiQIZKkBQsWNLm+VatWae/evZKkm266qcnnAwAAAACgrQqJYGDTpk1yuVxyOp1KT0+vsczgwYMlSevWrWtyffPmzfOf84wzzmjy+QAAAAAAaKtCIhjIzs6WJKWkpMjhcNRYpkePHgFlG6ukpMQ/6uDGG29s0rkAAAAAAGjrQmLxwYKCAklSu3btai3jO+Yr21gLFixQSUmJoqKidPXVVzfpXFXZ7W0/g7HZrAFfgYaiD8EM9COYgX4EM9CP0FT0IZihJfpRSAQD5eXlklTraAFJcjqdAWUbyzeN4Morr1RsbGyTzuVjtVqUmBhjyrlaQnx8VGs3ASGOPgQz0I9gBvoRzEA/QlPRh2CG5uxHIREMRERESJIqKipqLeNyuQLKNsbevXu1atUqSeZOI/B6DRUWlph2vuZis1kVHx+lwsJSeTze1m4OQhB9CGagH8EM9COYgX6EpqIPwQyN6Ufx8VENGmEQEsFAMNMEgpluUJ8PP/xQhmGoa9eu/l0OzOJ2h84LgcfjDan2ou2hD8EM9COYgX4EM9CP0FT0IZihOftRSEx26dmzpyQpNze31lEDe/bsCSjbUIZh6MMPP5QkjRo1ShaLpVHnAQAAAAAglIREMNC3b185HA65XC5lZWXVWGb16tWSpAEDBjSqjpUrV2rfvn2yWCzsRgAAAAAACBshEQzExsZq2LBhkqS5c+dWO75r1y5lZmZKkjIyMhpVh2/RwfPPP1/du3dvZEsBAAAAAAgtIREMSNKECRNksVg0f/58zZkzR4ZhSJIOHTqkiRMnyuv1asSIEUpLSwt43vDhwzV8+HAtWLCg1nMXFxdr4cKFkqSbbrqp+S4CAAAAAIA2JmSCgfT0dD388MOSpMmTJ+uyyy7TjTfeqMsvv1wbNmxQr1699Pjjj1d7Xk5OjnJyclRSUvuuAAsXLlRJSYmio6N11VVXNds1AAAAAADQ1oTErgQ+Y8eOVWpqqqZPn66srCwdPXpUKSkpysjI0Lhx4xQTE9Oo8/qmEVx11VWNPgcAAAAAAKEopIIBSRo6dKiGDh0adPktW7bUW2bWrFlNaRIAAAAAACErZKYSAAAAAAAA8xEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIzZW7sBDZWZmakZM2Zo3bp1KikpUUpKijIyMjRu3DhFR0c36pyGYejTTz/VvHnztGnTJhUWFiohIUG9e/fWJZdcol/84hcmXwUAAAAAAG1DSI0YmDVrlsaOHaslS5YoIiJCvXv3Vk5OjqZNm6abb75Z+fn5DT5ncXGx7rrrLv3ud7/T119/rejoaKWlpcnhcGjVqlV67bXXzL8QAAAAAADaiJAZMbB+/Xo99dRTkqTHHntMo0ePlsVi0cGDBzV+/Hht2LBBkyZN0tSpU4M+p2EYuv/++7V8+XL98Ic/1OTJk9WjRw//8cLCQq1atcr0awEAAAAAoK0ImREDL7/8srxer0aOHKkxY8bIYrFIkpKTk/Xss8/KarVq0aJF2rx5c9Dn/OCDD7Rs2TKdd955euWVVwJCAUmKj4/X5Zdfbup1AAAAAADQloREMFBcXKylS5dKkkaPHl3teM+ePTVkyBBJ0oIFC4I+78yZMyVJ48ePl90eMoMnAAAAAAAwTUi8G960aZNcLpecTqfS09NrLDN48GAtX75c69atC+qce/bs0datW2W1WnXRRRdp3bp1ev/997Vnzx5FR0drwIABuvnmm5WUlGTmpQAAAAAA0KaERDCQnZ0tSUpJSZHD4aixjG8agK9sfdavXy9JSkhI0DvvvKO///3vMgzDf/zzzz/X66+/rqlTp/pHIwAAAAAAcLoJiWCgoKBAktSuXbtay/iO+crW59ChQ5IqFxh85pln9KMf/Uj/7//9P/Xo0UPZ2dl66qmnlJmZqfvvv18ff/yxOnfu3KRrsNvb/qwNm80a8BVoKPoQzEA/ghnoRzAD/QhNRR+CGVqiH4VEMFBeXi5JtY4WkCSn0xlQtj4lJSWSJLfbrR49eujFF1/0nz81NVWvvPKKrrjiCh0+fFhvvfWWHnrooUa332q1KDExptHPb2nx8VGt3QSEOPoQzEA/ghnoRzAD/QhNRR+CGZqzH4VEMBARESFJqqioqLWMy+UKKBvsOSXp9ttvrxY6REVF6ZZbbtHUqVO1dOnSJgUDXq+hwsKSRj+/pdhsVsXHR6mwsFQej7e1m4MQRB+CGehHMAP9CGagH6Gp6EMwQ2P6UXx8VINGGIREMBDMNIFgphtUFR8f77/du3fvGsv4Ht+3b19Q56yL2x06LwQejzek2ou2hz4EM9CPYAb6EcxAP0JT0YdghubsRyEx2aVnz56SpNzc3FpHDezZsyegbH3OPPNM/+3apij4RhV4vfxHDAAAAAA4PYVEMNC3b185HA65XC5lZWXVWGb16tWSpAEDBgR1zn79+ikyMlKStHfv3hrL+MKGpi48CAAAAABAW9XswYDH49E///lPjR8/Xvfee6/ee++9Bp8jNjZWw4YNkyTNnTu32vFdu3YpMzNTkpSRkRHUOaOionTZZZdJkj788MNqxw3D0Lx58ySJ7QoBAAAAAKctU4KBf//73+rbt69+85vfVDs2ceJEPfnkk1qyZIk+//xzTZ48Wb/97W8bXMeECRNksVg0f/58zZkzR4ZhSKrcdnDixInyer0aMWKE0tLSAp43fPhwDR8+XAsWLKh2zvvuu092u13ffvutXnrpJXk8HkmVOxX87W9/0+bNmxUREaGxY8c2uL0AAAAAAIQCU4KBZcuWSZKuu+66gMdXrFihhQsXyjAMDRw4UD/4wQ8kSQsWLNDixYsbVEd6eroefvhhSdLkyZN12WWX6cYbb9Tll1+uDRs2qFevXnr88cerPS8nJ0c5OTn+7QmrOuuss/TEE0/IZrPphRde0LBhw/STn/xEF198sd588005HA499dRTAesRAAAAAABwOjElGNi0aZMkadCgQQGP+4bojx49WrNnz9b06dN1//33BwzTb4ixY8dqxowZuuSSS1RaWqrt27crJSVFv/rVr/T+++8rKSmpwee88cYbNWfOHGVkZMhqtWrTpk1yOBy67rrr9O9//7ta2AEAAAAAwOnElO0Kjx07JqfTWe2N+TfffCOLxaKf/vSn/sduv/12vfDCC1q/fn2j6ho6dKiGDh0adPktW7bUW+bcc8/V888/36j2AAAAAAAQykwZMVBcXOzf2s/n0KFDOnDggNq3b68+ffr4H2/Xrp1iY2OVl5dnRtUAAAAAAKAJTAkGYmNjVVRUpNLSUv9jq1atkiQNHDiwxuecGiQAAAAAAICWZ0ow4BsR8Nlnn/kf+/DDD2WxWHTBBRcElC0qKtLx48fVoUMHM6oGAAAAAABNYMoaA9ddd51WrVqlxx57TOvWrdORI0e0dOlSOZ1OXX311QFl16xZI0nq2bOnGVUDAAAAAIAmMCUYuPnmm7Vw4UItX75cc+fOlWEYslgs+s1vfqOOHTsGlF2wYEGNIwkAAAAAAEDLMyUYsNlseuONN/TJJ59ozZo1io+P1yWXXKLBgwcHlHO5XDp8+LDOP/98XXLJJWZUDQAAAAAAmsCUYECSrFarbrjhBt1www21lnE6nXr99dfNqhIAAAAAADSRKYsPAgAAAACA0GTaiIG6fPnll1q2bJmsVqsuvfRSXXzxxS1RLQAAAAAAqIcpIwYWLVqkyy+/XJMnT652bMqUKZowYYLeeecdzZo1S3fffbf+8pe/mFEtAAAAAABoIlOCgS+++EK5ubk6//zzAx7fsGGD3nrrLRmGoS5duqhHjx4yDEMzZ87UihUrzKgaAAAAAAA0gSnBwPfffy9JGjp0aMDj77//viTpiiuu0OLFi7Vw4ULdfvvtMgxDc+fONaNqAAAAAADQBKYEA3l5ebLZbOrYsWPA48uWLZPFYtE999wjq7Wyql/+8peSpLVr15pRNQAAAAAAaAJTgoGioiLFxMQEPHbs2DHt3r1b8fHxSk9P9z/eqVMnRUVF6fDhw2ZUDQAAAAAAmsCUYCA6OlpFRUWqqKjwP7Z69WpJ0oABA6qVdzgcstlsZlQNAAAAAACawJRg4Mwzz5RhGPrqq6/8j3322WeyWCwaPHhwQNnS0lIVFRVVm3YAAAAAAABant2Mk1xxxRVau3at/vSnP2nnzp06fPiw/vOf/8hqterqq68OKPv999/LMAx169bNjKoBAAAAAEATmBIM3HHHHfroo4+0ZcsWPffcczIMw/949+7dA8ouWrRIFoul2taGAAAAAACg5ZkSDERERGj27Nl66623tHbtWsXFxemyyy7TddddF1DO5XJp1apV6tKli4YNG2ZG1QAAAAAAoAlMCQYkKSYmRhMmTKizjNPp1Pz5882qEgAAAAAANJEpiw8CAAAAAIDQZNqIgaqOHz+ujRs36ujRo5Kk9u3bq1+/foqNjW2O6gAAAAAAQCOZGgz4Fh9cunSpvF5vwDGr1apLL71Uv/71r5WammpmtQAAAAAAoJFMm0qwaNEijR49Wl999ZU8Ho8Mwwj45/F49OWXX2r06NH673//a1a1AAAAAACgCUwZMbB37149+OCDcrlc6tq1q+6++25dfPHF6ty5syTpwIEDWrZsmd58803t27dPDz74oD755JNqWxkCAAAAAICWZcqIgTfffFMul0sDBgzQRx99pFtvvVU9evSQ0+mU0+lUjx49dOutt+qjjz7SgAED5HK5NGPGDDOqBgAAAAAATWBKMPDNN9/IYrHo0UcfVUxMTK3loqOj9eijj8owDC1btsyMqgEAAAAAQBOYEgwcOHBAMTExQS0qmJqaqtjYWB04cMCMqgEAAAAAQBOYEgzY7Xa53e6gyhqGoYqKCtntzbJTIgAAAAAAaABTgoEzzjhD5eXlWrp0ab1lly5dqvLycp1xxhlmVA0AAAAAAJrAlGBg+PDhMgxDkyZN0o4dO2ott337dk2ePFkWi0WXX365GVUDAAAAAIAmMGU8/9ixY/Xee+/pwIEDGjVqlDIyMjR06FAlJydLqlyD4JtvvtHChQtVUVGhzp0762c/+5kZVQMAAAAAgCYwJRiIjY3VG2+8oV/96lfKycnRJ598ok8++aRaOcMw1K1bN02bNk2xsbFmVA0AAAAAAJrAtBUA+/Tpo48++kjvvPOOFixYoC1btsjj8UiSbDabUlNTdc011+jWW2+tc0tDAAAAAADQckzdGiAmJkbjxo3TuHHjVFFRoYKCAklSu3bt5HA4JElFRUW68cYbZbFY9MEHH5hZPQAAAAAAaKBm2zPQ4XCoQ4cO1R53u93atGmTLBZLc1UNAAAAAACCZMquBAAAAAAAIDQRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwliz7UrQXDIzMzVjxgytW7dOJSUlSklJUUZGhsaNG6fo6OgGnevhhx/WvHnz6izz+uuv65JLLmlKkwEAAAAAaLNCKhiYNWuWnnzySRmGoc6dO6tLly7avn27pk2bpkWLFmn27NlKSEho8Hm7dOmiLl261HisXbt2TWw1AAAAAABtV6OCgb59+5rdjnqtX79eTz31lCTpscce0+jRo2WxWHTw4EGNHz9eGzZs0KRJkzR16tQGn/vHP/6x7r//frObDAAAAABAm9eoNQYMw2jSv8Z4+eWX5fV6NXLkSI0ZM0YWi0WSlJycrGeffVZWq1WLFi3S5s2bG3V+AAAAAADCUaNGDNx3331mt6NOxcXFWrp0qSRp9OjR1Y737NlTQ4YM0fLly7VgwQKlpaW1aPsAAAAAAAhVIREMbNq0SS6XS06nU+np6TWWGTx4sJYvX65169Y1+PwrVqzQtm3blJ+fr/j4ePXv31833HCDunbt2tSmAwAAAADQpoXE4oPZ2dmSpJSUFDkcjhrL9OjRI6BsQ6xatSrg/n//+1+99NJL+vWvf6177rmnwecDAAAAACBUhEQwUFBQIKnuHQJ8x3xlg3HGGWfo4Ycf1pAhQ9S1a1c5nU5t2bJF06dP14IFC/TMM88oOjpat99+e9MuQJLd3qjlHFqUzWYN+Ao0FH0IZqAfwQz0I5iBfoSmog/BDC3Rj0IiGCgvL5ekWkcLSJLT6QwoG4zx48dXe+y8887T888/r0cffVSzZ8/WP/7xD40aNUoxMTENbPVJVqtFiYmNf35Li4+Pau0mIMTRh2AG+hHMQD+CGehHaCr6EMzQnP0oJIKBiIgISVJFRUWtZVwuV0DZppo4caLee+89FRYWKjMzU5dffnmjz+X1GiosLDGlXc3JZrMqPj5KhYWl8ni8rd0chCD6EMxAP4IZ6EcwA/0ITUUfghka04/i46MaNMIgJIKBYKYJBDPdoCHi4uLUp08fbdy4Ubt3727y+dzu0Hkh8Hi8IdVetD30IZiBfgQz0I9gBvoRmoo+BDM0Zz8KickuPXv2lCTl5ubWOmpgz549AWXN4Ju64Ha7TTsnAAAAAABtSUgEA3379pXD4ZDL5VJWVlaNZVavXi1JGjBggCl1ut1u7dy5U5LUuXNnU84JAAAAAEBbExLBQGxsrIYNGyZJmjt3brXju3btUmZmpiQpIyPDlDrnzJmjoqIi2e12DRkyxJRzAgAAAADQ1oREMCBJEyZMkMVi0fz58zVnzhwZhiFJOnTokCZOnCiv16sRI0YoLS0t4HnDhw/X8OHDtWDBgoDHly1bpr/97W/atWtXwOMul0uzZs3SlClTJEm33HKLOnXq1HwXBgAAAABAKwqJxQclKT09XQ8//LCefvppTZ48WdOmTVNiYqK2b98ul8ulXr166fHHH6/2vJycHElSSUngrgClpaV644039MYbb6hDhw5KTk6WJGVnZ/vLXnXVVXrooYea+coAAAAAAGg9IRMMSNLYsWOVmpqq6dOnKysrS0ePHlVKSooyMjI0btw4xcTEBH2u/v37a8KECVq7dq12796t7OxsVVRUKCkpScOGDdONN96o4cOHN+PVAAAAAADQ+kIqGJCkoUOHaujQoUGX37JlS42Pd+nSRb/+9a/NahYAAAAAACEpZNYYAAAAAAAA5iMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAAAACEMYIBAAAAAADCGMEAAAAAAABhjGAAAAAAAIAwRjAAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgLuWAgMzNTv/zlLzVkyBClp6crIyND//jHP1RSUmLK+d955x2lpqYqNTVVP/3pT005JwAAAAAAbVVIBQOzZs3S2LFjtWTJEkVERKh3797KycnRtGnTdPPNNys/P79J5z948KCeffZZcxoLAAAAAEAICJlgYP369XrqqackSY899piWLFmiefPmafHixerfv7927NihSZMmNamOP//5zyotLdVll11mRpMBAAAAAGjzQiYYePnll+X1ejVy5EiNGTNGFotFkpScnKxnn31WVqtVixYt0ubNmxt1/v/85z/64osvdPvtt6t///5mNh0AAAAAgDYrJIKB4uJiLV26VJI0evToasd79uypIUOGSJIWLFjQ4PMXFBToySefVOfOnfWb3/ymSW0FAAAAACCUhEQwsGnTJrlcLjmdTqWnp9dYZvDgwZKkdevWNfj8Tz/9tI4cOaJJkyYpJiamSW0FAAAAACCUhEQwkJ2dLUlKSUmRw+GosUyPHj0Cygbrm2++0QcffKDhw4drxIgRTWsoAAAAAAAhxt7aDQhGQUGBJKldu3a1lvEd85UNRllZmSZPnqzo6GhNnjy5aY2sh93e9jMYm80a8BVoKPoQzEA/ghnoRzAD/QhNRR+CGVqiH4VEMFBeXi5JtY4WkCSn0xlQNhgvvPCC9uzZo0ceeURdunRpWiPrYLValJgYOlMU4uOjWrsJCHH0IZiBfgQz0I9gBvoRmoo+BDM0Zz8KiWAgIiJCklRRUVFrGZfLFVC2Phs3btRbb72lfv366ac//WnTG1kHr9dQYWFJs9ZhBpvNqvj4KBUWlsrj8bZ2cxCC6EMwA/0IZqAfwQz0IzQVfQhmaEw/io+PatAIg5AIBoKZJhDMdIOq/vjHP8rr9eqxxx6TzWZreiPr4XaHzguBx+MNqfai7aEPwQz0I5iBfgQz0I/QVPQhmKE5+1FIBAM9e/aUJOXm5qqioqLGKQV79uwJKFufjRs3ymaz6Ve/+lW1YyUllZ/ur1mzRhdffLEk6d///nezTjcAAAAAAKA1hEQw0LdvXzkcDrlcLmVlZfm3Jqxq9erVkqQBAwYEfV6Px6MjR47UeryiosJ/3OPxNKzRAAAAAACEgJAIBmJjYzVs2DB9+eWXmjt3brVgYNeuXcrMzJQkZWRkBHXOLVu21Hps6tSpevHFF3XhhRdq1qxZjW84AAAAAABtXMjsmzFhwgRZLBbNnz9fc+bMkWEYkqRDhw5p4sSJ8nq9GjFihNLS0gKeN3z4cA0fPlwLFixojWYDAAAAANCmhUwwkJ6erocffliSNHnyZF122WW68cYbdfnll2vDhg3q1auXHn/88WrPy8nJUU5Ojn/dAAAAAAAAcFJITCXwGTt2rFJTUzV9+nRlZWXp6NGjSklJUUZGhsaNG6eYmJjWbiIAAAAAACHFYvjG5KPZeDxe5eUVt3Yz6mW3W5WYGKNjx4rZTgWNQh+CGehHMAP9CGagH6Gp6EMwQ2P6UVJSjGy24CcIhMxUAgAAAAAAYD6CAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGAup7QrRfLxeQ5t25aki+5gcFkO9U9rJarW0drMAAAAAAM2MYABaveWQZi/epmNF5f7HEuMidNuIPhqc2qkVWwYAAAAAaG5MJQhzq7cc0kvz1geEApJ0rKhcL81br9VbDrVSywAAAAAALYFgIIx5vYZmL95WZ5l3F2+T12u0UIsAAAAAAC2NYCCMbd2bX22kwKnyisq1dW9+yzQIAAAAANDiCAbCWH5x3aFAQ8sBAAAAAEIPwUAYS4iJMLUcAAAAACD0EAyEsbO7Jygxru43/UlxETq7e0LLNAgAAAAA0OIIBsKY1WrRbSP61Fnm1hF9ZLVaWqhFAAAAAICWRjAQ5gandtK9N55T68gBi4VQAAAAAABOZ/bWbgBa3+DUThrYp6N25BaowrDIYTH03dbDWrhyr2Z+tlm9usTXO+UAAAAAABCaGDEASZXTCvr2TNKlg7qpb88k3XRJb/VIjtXx0gpN/88meQ2jtZsIAAAAAGgGBAOokcNu1bjr+8tpt2pDdp4Wf7uvtZsEAAAAAGgGBAOoVUqHGI25vHJxwn8v2a69h463cosAAAAAAGYjGECdfjQgRQPO6iC3x9BrH22Qq8LT2k0CAAAAAJiIYAB1slgsGntNmuJjnMo5Uqz3luxo7SYBAAAAAExEMIB6xUc79Ytr+0qSPl+9T1k7jrRyiwAAAAAAZiEYQFDOPbO9RgzuJkma/ukmFRa7WrlFAAAAAAAzEAwgaD+5rLe6doxRYUnlFoYGWxgCAAAAQMgjGEDQHHabfnl9f9ltVmXtOKov1+S0dpMAAAAAAE1EMIAG6dYpVj/5UW9J0pwvtiv3SHErtwgAAAAA0BQEA2iwy8/vpnN6JanC7dVrH21Qhdvb2k0CAAAAADQSwQAazGqx6K5r+yo2yqE9h45r3v92tnaTAAAAAACNRDCARkmIjdDPr0mTJC1YuUcbduW1cosAAAAAAI1BMIBGG9ino340sKsk6c1PNup4aUUrtwgAAAAA0FAEA2iSMcPPUuekaOUfd2nmZ5vZwhAAAAAAQgzBAJokwmHTL2/oL5vVou+2HtbSrP2t3SQAAAAAQAMQDKDJzugcp5suOVOSNHvxVh3MK2nlFgEAAAAAgkUwAFNcdVEPpfVIkKvCq9c+3iC3hy0MAQAAACAUEAzAFFaLRXdf108xkXZl7y/SR8uyW7tJAAAAAIAgEAzANEnxkbozo3ILw0+X79aWPcdauUUAAAAAgPoQDMBUF6R10rBzu8iQ9MYnG1VSxhaGAAAAANCWEQzAdLeO6KNOCVE6WliutxduYQtDAAAAAGjDCAZguqgIu+65oZ+sFotWbjqkzA0HW7tJAAAAAIBaEAygWfROaacbhvWUJM1atEWH80tbt0EAAAAAgBrZW7sBDZWZmakZM2Zo3bp1KikpUUpKijIyMjRu3DhFR0c36Fxz5szRmjVrtHHjRh05ckQFBQWKiorSmWeeqSuuuEJ33HGHoqKimulKTn/XDj1D67PztH1fgV7/eKMeun2gbFayKAAAAABoS0LqXdqsWbM0duxYLVmyRBEREerdu7dycnI0bdo03XzzzcrPz2/Q+f72t79p3rx52r17t2JjY5WWlqbIyEitW7dOzzzzjEaNGqX9+/c3z8WEAZvVqnHX9VNUhE3bcwr06Te7W7tJAAAAAIBThEwwsH79ej311FOSpMcee0xLlizRvHnztHjxYvXv3187duzQpEmTGnTO++67T++9957WrFmjRYsW6f3339fXX3+t2bNnq1OnTtq1a5f+7//+rzkuJ2x0SIjSHVemSpI++nqXduQUtHKLAAAAAABVhUww8PLLL8vr9WrkyJEaM2aMLBaLJCk5OVnPPvusrFarFi1apM2bNwd9zrFjxyo9PV3WU4a3Dx48WI888ogkaenSpSopKTHvQsLQ0P6dNaRfsryGodc+3qDScndrNwkAAAAAcEJIBAPFxcVaunSpJGn06NHVjvfs2VNDhgyRJC1YsMCUOnv37i1J8nq9Ki8vN+Wc4eyOK89W+/hIHc4v0+zFW1u7OQAAAACAE0IiGNi0aZNcLpecTqfS09NrLDN48GBJ0rp160ypc/Xq1ZKkrl27KjEx0ZRzhrPoSIfuub6fLBZp2fcHtHITWxgCAAAAQFsQErsSZGdnS5JSUlLkcDhqLNOjR4+Aso3hdrt16NAhLV68WM8995wcDof+8Ic/NPp8CHR29wRdO7SnPlm+S28v2KIzu8TrSEGZ8ovLlRATobO7J8hqtbR2MwEAAAAgrIREMFBQULlgXbt27Wot4zvmK9sQTz75pN5+++2Ax4YNG6b7779fAwYMaPD5amK3t/3BGTabNeBrc7jp0jO1cVeeduYW6g+vZ8rtMfzHkuIidPtVqbogrVOz1Y/m1RJ9CKc/+hHMQD+CGehHaCr6EMzQEv0oJIIB3xz/2kYLSJLT6Qwo2xDdu3fXoEGD5HK5lJubq7y8PH333Xf66KOP1K9fP/+5G8tqtSgxMaZJ52hJ8fFRzXr+K4acoVc/+D4gFJCkvKJyTf13lh752QX6QXpKs7YBzau5+xDCA/0IZqAfwQz0IzQVfQhmaM5+FBLBQEREhCSpoqKi1jIulyugbEPceeeduvPOO/33v/32Wz366KN65513lJubq1deeaXB56zK6zVUWNj2dzaw2ayKj49SYWGpPB5vs9Th9Rp67791Lz746rwspXaNZ1pBCGqJPoTTH/0IZqAfwQz0IzQVfQhmaEw/io+PatAIg5AIBoKZJhDMdINgnX/++Xrttdd0xRVX6Msvv9Tq1av9ixs2ltsdOi8EHo+32dq7efcx5RXVPaojr7BcG7PzlHYGiz6GqubsQwgf9COYgX4EM9CP0FT0IZihOftRSEx26dmzpyQpNze31lEDe/bsCSjbVF26dNHZZ58tSdqwYYMp54SUXxzcVI9t+/JlGEb9BQEAAAAATRISwUDfvn3lcDjkcrmUlZVVYxnf9oJmLRYoSR6PJ+Armi4hJripHvOWZuvRGav05Xf7VFLmbuZWAQAAAED4ColgIDY2VsOGDZMkzZ07t9rxXbt2KTMzU5KUkZFhSp27du3S1q2Vc+H79u1ryjlRuWVhYlzd4YDTbpXNatGeQ8c1a9FWTXzpa834zybtzC1kFAEAAAAAmCwkggFJmjBhgiwWi+bPn685c+b43yAeOnRIEydOlNfr1YgRI5SWlhbwvOHDh2v48OFasGBBwOOfffaZ3n77bR0+fLhaXZmZmbrnnnvk9XrVr18/XXjhhc13YWHGarXothF96ixzz/X99Nz9w3TL5X3UpX20XBVeLc3aryfe/lZ/nrFKXzCKAAAAAABMYzFC6CPYmTNn6umnn5ZhGOrSpYsSExO1fft2uVwu9erVS7Nnz1ZSUlLAc1JTUyVJU6ZM0U033RRwrilTpkiqXE+gQ4cOMgxDOTk5OnbsmCTprLPO0uuvv66UlKZtnefxeJWXV9ykc7QEu92qxMQYHTtW3OyLo6zeckizF2/TsSoLESbFRejWEX00OLWT/zHDMLRtX4G+Wpurb7ccUsWJdjntVl3YN1mXDkjRmSnxsljYwaAtaMk+hNMX/QhmoB/BDPQjNBV9CGZoTD9KSoo5/XYl8Bk7dqxSU1M1ffp0ZWVl6ejRo0pJSVFGRobGjRunmJiYoM81YsQIlZeXa+XKlcrOztb27dvldruVmJioSy65RFdeeaVGjhwpp9PZjFcUvgandtLAPh21dW++8ovLlRATobO7J1TbotBisejs7gk6u3uCbh3RR99sOKD/rc1VzpFiff39fn39/X516xijSwd01dD+yYqOdLTSFQEAAABAaAqpEQOhihED5jIMQztyCvXV2hyt3Bw4iuCCtE66dEBX9e5a8ygCr9eoN4xA44VKH0LbRj+CGehHMAP9CE1FH4IZGDEA1MBiseisbu10Vrd2umVEH2VuOKgla3OUc7hYy9Yf0LL1B9S1Q4wuGZCiH5zTWTEnRhHUNH0hMS5Ct50yfQEAAAAAwgkjBloAIwaan2EY2plbqK/W5mrlpoNynWi/w27V+amd1DkpSvOWZtf6/HtvPIdwwASh3IfQdtCPYAb6EcxAP0JT0YdgBkYMAEGyWCzq3bWdendtp1su76PMjQe0ZE2u9h0+rm82HKj3+e8u3qaBfToyrQAAAABA2CEYwGknOtKu4YO66bKBXZW9v0jzv96p73fm1fmcvKJybd2br7QzEluolQAAAADQNgQ/tgAIMRaLRWemxGvoOZ2DKv/Zit1aveWQCopdzdwyAAAAAGg7GDGA015CTERQ5b7fmecfWdApIcq/wGGfru3UpUOMrDXscgAAAAAAoY5gAKe9s7snKDEuImA3glPFRNp1flon7cgpUM7hYh3KL9Wh/FItX1+5PkF0hF1ndatcw6BP13bqlRKvCIctqPrZIhEAAABAW0YwgNOe1WrRbSP66KV562stM/bqNP+uBCVlFdqRW6ht+wq0fV++du4vVEm5W1k7jiprx1FJks1qUY/kWJ3VNUF9TowsSIitPjKBLRIBAAAAtHVsV9gC2K6wbajpTXpSXIRuredNutvj1d5Dx7V9X4G25VSGBfnHq69D0KFd5ImQIEF9urbT/rwSTfuw9jDidNwi8XTvQ2gZ9COYgX4EM9CP0FT0IZiB7QoBEw1O7aSBfTo2eFi/3WZVry7x6tUlXldc0F2GYehoQZm25/iCggLtO3RcRwrKdKSgTN9sOBhUe9giEQAAAEBbQDCAsGK1Wpq8JaHFYlGHhCh1SIjSkP6VOx6Ulru1I7cyJKicglCgCk/daV5eUbnm/W+nzuvTQZ0SoxQX5ZDFpAUOWdcAAAAAQLAIBgATREXYdU6v9jqnV3tJ0vL1+/XGJ5vqfd6nmbv1aeZuSVKk06ZOiVHqlBClTonRVW5HKSEuIuhdEVjXAAAAAEBDEAwAzSApLjKoct07xai4zK1jheUqc3m05+Bx7Tl4vFo5h92qjgkng4KOCVFKToxSx8QotY+PlP3E/KHVWw7VuMjisaJyvTRv/Wm5rgEAAACApiEYAJpBMFskJsVF6P/GXiir1aIKt0eH88t06FjlNomHjpWc+FqqowVlqnB7lXukWLlHqi9iabVY1KFdpDokRGpHTkGd7WrOdQ28XkObduWpIvuYHBZDvVPaMX0BAAAACAEEA0AzCGaLxFtH9PG/cXbYbUrpEKOUDjHVynm8Xh0tLNfhY4GBwaH8Uh0+ViqX21v5WH5pve3KKyrX6x9vVO+u8UqIjVBCXIQSYp1qFxMhhz34VUtPxfQFAAAAIHSxXWELYLvC8NXYLRKD5TUMFRx36dCxEmVuOKiv1uU2+lyxUY4TYYGz8mtshBJjnVUChAjFxzhkswYGCLVNX/BpzukLLLJ4euK1CGagH8EM9CM0FX0IZmC7QiDENXaLxGBZLRYlxkUoMS5ChqGggoFBZ3eQ1WpVflG58o9X/nN7DB0vrdDx0grtO1z7cy2S4mN8wYFT7WKdWrnpUJ31Ndf0BUYpAAAAAOYgGACamRlbJAYj2HUNJow6N+BNumEYKi5z+4OCY8fLlX/cVRkaFJ28XXDcVTlCodilgmKXdh8Mrl15ReX60xsr1L5dpKIj7IqOtAd8jYq0KzrCUe1xh91a6/aNrb3IIiMVAAAAcDohGABOEw1d18DHYrEoNsqh2CiHunWKrfW5XsNQUUlFwEiD9dl5Wr2ljiEGJxzIK9GBvJLgL0aS3WY5ERw4/IFBVIRdUU6bVm6ue5TC7GZcZJGRCgAAADjdsMZAC2CNAbSk5l7XoKrNu4/pr++uqbfcjZf0UlJcpErK3Sotc6uk3K0S/9cKlZZ7VFJe4X/MjFelSKdNCbER/tAjJspe5bZDsZGOwPtRjnoXYGQ9hZbBaxHMQD+CGehHaCr6EMzAGgMAGqy51zWoKtjpC9cO6Rl0/YZhqMzlUWlAeOD2Bwdb9+br2yBGKZS5PA0epRDhsCk2yu4PCqqGCNGRdn2yfFedzz/d1lMIpzACAAAgnBEMAKehllrXoLHTF+pisVgqpwxE2JUUX/14t46xQQUDP78mTZ0SovyLKh4vrVBxqfvk/bIKFVd53GsYKq/wqLzCo6OFtQcddckrKtfkN1coKT7y5NSHiBNrKVT96j9mU3SEXZERdlnb2HoKTJkAAAAIHwQDAJpkcGon3XvjOS02fSHYUQoXn9Ml6EDCaxgqK/eFBu4TYUFFQIiw50CRduQW1nuu3KMlyj3asJEKFkmREbaA8MC3nsLa7UfqfO47/92qvmckKirCXutijQ3Vmos7er2GNu3KU0X2MTkshnqntGOUAgAAQDMjGADQZL7pCztyC1RhWJr1DV1zjFKwWiyKjnQoOtKhTrUMtAh2PYVRP+yl9vGRKilzV06HKA/8Wnnbo9KyCpWUe+T2eGVIKi33qLTcozw1bLRC/nGX7vvHUlktFkWdEi5UHbXgezxwtILDP2ohOtIuh90mr9fQ7MXb6qyTKRMAAACnF4IBAKawWi3q2zOpRRbYaelRClLwIxWuGxr8egqSVOH2Vg8QTqytsHn3MWVuDG5fSO+JbSeLy9xB130qu80ih92q0nJPneXyiso198vtOqNznCIctsp/zlO+OqxyOmy1TpE4FVMmAAAAWg/BAICQ1JKLLErNM1JBkhx2qxx2p+JjnNWOdUqICioY+O1PzlO3TrFVRiSc8rXs5GiF0nKPSk6MViitMorBkOT2GHJ76g4FfBat2htUOafDqkiHTc4ToYH/tsOmSKfvtlVLs/bXeZ53/rtV5/RqrwinLah6g9HaUyYYpQAAANoKggEAIaulFln0aavrKfTvlSSr1aLEuIhG1eM1DJWf2AliQ3aeZny2ud7nnNU1Xk6HrXLBRpfnxMKNXv9tH1eFV64Kr6SKRrXNJ/+4S+Of/UoOuzVgHQbf1Iho3xSKyCq3TzkeFWFXZETlKIZwnDIhEUgAAICaEQwAQAO05EiF5hqlUK2eKjtBXHxuF334dXa9YcTDtw+utV6vYaiiwqvyCo/KKjxynQgLfLfLTuz+4AsRsvcXat32o0G1tcLtVYHbpYJiV6OutXKhR7scNosKS+oOK/KKyjV/WbZ6p8SfGOFgV6TzxMiHE6Mdgp0q4dOaoxSYNgEAAGpDMAAADdSSIxVaepSCGWGE1WKpXGvAaVMNO05Ws3n3saCCgQd+fK66dYwNmCpRbZHHMnfAlIqTCz265fYYJxZ6dKs0iHZJ0sfLdtV53DclwjdNovK23R8eRDoCg4T5X2fXeb7mHKXAtAkAAFAbggEAaONaej2FtjplIr13hyZdc4Xb419bYeOuPP1z0dZ6n9O9Y4ysNqvKXR6VudyVIx9cHhlG5fHyE6MfVNzoZgXIKyrX+L9/pahIuz9U8C3oWPV+ZNVFH6sEEBGOwPuRzsrREeE2bYJtLwEAaBiLYfj+vEFz8Xi8yssz6a/GZmS3W1tkRXmcvuhDp5eW/KS3tk+0fcz+RNvrNfT/pi2vN4z46/gfVLtmwzDkcleup1BW4VFZuds/NaLM/+9kiFDmqjyWc7RY2bmFpl2D2Qad3UFd2sdUjnqoEiz4R0VUGR3hO17XVIqW/plWrZcpEzALv9fQVPQhmKEx/SgpKUY2mzXoOggGWgDBAMIFfQhNUdMbuubcgrKl37hu3n1Mf313Tb3l7rm+r7p2iJWrwquyCrc/cCivsjaD/75v/QZXDccqPKpo5v8OfbtOVIYFJ9dgiLBbtT47T6466o+Pduj/3TpQsVEORUbY5bRbZWngmg2naq0wQmLKxOmK32toKvoQzNASwQBTCQAAbYJvysSO3AJVGJZmHwLeVqdMXNS3s2nX7PF69f2OPL3wfla9ZYf0S1ZslKNycciqgYNvwcgaplL4d52oZyHHmhSWVGjSmyv9921WiyKdNv9CmL7dJCIjTjzmtCsqosrxU+5HOGxhOWWCMAIAYAaCAQBAm2G1WtS3Z1KLfbpyOu4yUZXNalV67/ZBBRJ3X9cvqLoNw1CF23syQPAHBydHLWzac0xfZ+2v91wOu1Vut1eGJI/XUHGZW8Vl7oZcYoPkFZXrlfnrldIhxj9VouoCkTU9Zq/n05bWWtiRKRMAADMRDAAAwtrpvMuEZH4gYbFY5HRU7rKg6JrLJMZFBBUM/PYn5ym1R4J/LYaScrfKquwqUebyqKTMrTLXifsnFo+sPFa564SvfF3TFqr6dsthacvhoMpKkt1mqXG7ygiHTREOq9ZsO1Ln82ct3KqU9jGKjrT712dorikT7DIBAGgsggEAAFpQS+8y4auzLU6bOLt7giwWi386QGJcRKPr3JB9VH+fs67echf27aSYKIfKyn0jHCoDiLJTFpB0eyqDBrfHkNvT+JEMhSUu/fGNFf77FqnKCIXKsCHKWT14qOl+lNMmh92qWfXsqMGUCQBAQxEMAADQwlpylILP6T5tou8ZSUGFEeOu7x9UvW6Pt9puE+Uud0CAsHVvvjI3Hqz3XA67RW63IUOSIfnPJ7mCv8AGyCsq15R/rlaHhCj/bhJVw4aA2xG2aiMiatptgikTAHB6IxgAACBMnM7TJswOI+w2q+w2q2IiHbWW6ZwUHVQw8NufDFBqj4TKnSZOhAtl5ZXbWpZV2UmirNwdsOVlTeWKSlxBjV7YkVuoHY3cHvPUMMHpsGrX/qI6n/PWgi2KctoVHWUPCCCaMnWiNadMSJUjFTbtylNF9rFmXwwVAFobwQAAAGgWLT1toq1PmYg48Yl8uybUGey2l1dd2F2JsRGV4ULVEQ9Vg4cqoyHKXG7/bhO+7S4LGrDT8vHSCj0zZ221x6tOnag2YiGillEMzso1LN5euKXOOptryoTEtAkA4YdgAAAANJuWnjbRkttetsaUiWDDiJ/86KwG1evfbaKG0OD7nUf1+ep99Z4jIdYpi8Xif75hNO/Uibyick2evkKJcZF1TpeIirDXeqym71G4TZsgjAAgEQwAAIDTTEtuexnqUyZ8qu42ER/jDDjmtFuDCgbGXd/fHwIZhnFy6kSVUQmlVYOH8lNDiJO3jxaU6mhh7eGHT+6REuUeKWnQtQZcm8N6MkBw2hXhsCo7iGkTkU67oiNPBg6+6ReNfUNNGEEYAbS2kAsGMjMzNWPGDK1bt04lJSVKSUlRRkaGxo0bp+joWvZNqoHH41FmZqaWLFmiNWvWaNeuXSorK1NCQoLOPfdcjRkzRj/60Y+a70IAAMBpgSkTJ6dM+DR16kSwUyZGDeul9u0iq4908K/XUHPw4PFWzptwVXjlqnCpsIHTJv5ew7QJqTJEObmbhP3kNArHKferBApOh1X/bIWdJsItjABQN4th+GaUtX2zZs3Sk08+KcMw1LlzZyUlJWn79u1yuVzq3bu3Zs+erYSEhKDO9d577+lPf/qTJMlqtapHjx6KiYnR7t27dfz4cUnSmDFj9OijjzZ5v2GPx6u8vAb8xmkldru1RT5dwemLPgQz0I9ghnDoRy35qWttbyJ9zH4T6fUa+n/TltcbRvx1/A8afM2GYcjt8VaOXjhlwcfvd+Y1aNqEb4cKbwv8OR0f41BclFNOh00RDqsiHJUBROV93z+rIk6MfPA95nSectxhk91m1eTpK5vl+1uXlu5HVbXGKAWv12iRaU04/TXmd1pSUoxsNmvwdTS2cS1t/fr1euqppyRJjz32mEaPHi2LxaKDBw9q/Pjx2rBhgyZNmqSpU6cGfc7U1FT99Kc/VUZGhuLi4iRJbrdbb731lv72t79pzpw5SktL02233dYs1wQAANBY7DLRuPUbLBaLHHabHHab4qNPnTZha9S0CV/Q4AsKyl0elVW4VVZ+YseJKiMW/FtgVlQ+diS/VAfySuuts7C4QoXFFQ2+3sbKKyrXs3PXqlNClH+aSYTDKqf9RCBht/pDCWctj9ttFv8HbF6vodmLt9VZZ3MtKNkaoxQYGYFQEzIjBiZMmKDPP/9co0aN0l/+8peAY7t27dLVV18tr9er+fPnKy0trd7z5efnq127drWOBpg0aZLmzp2rtLQ0zZ8/v0ltZ8QAwgV9CGagH8EM9KPm0dKfutb05qq5wgipeUcq1CbYaRN3XHG2OreP9u8a4arwqvzEVpcnH/OovIbHy10nj7kqPGqpP/4tFvlDAskIKti45LwUdesYcyJwOBE6+AIIu9X/eMSJx+31fCLaGqMUWnNkhMQaDqcjRgycUFxcrKVLl0qSRo8eXe14z549NWTIEC1fvlwLFiwIKhiob8rBJZdcorlz5yo7O7tRbQYAADjdtNYuEy31Jqct7zTxo4FdTanXMAytz87Tc3PX1Vv2RwNSlBAbcTKIcFcGDK4Kb0AQ4XIHBg++NRwMQ5UhhcsTdPv+ty63Qddjs1r8IxacjpPBgdNuldNu1aY9+XU+f+Znm+Vye/3Bg8Nuld1ulcNWedthrzy377bNaqlzmnFrjoyQWFASjRcSwcCmTZvkcrnkdDqVnp5eY5nBgwdr+fLlWreu/he5YJSVlUmSoqKiTDkfAAAAGq41wojTZdpETSwWi/r3TAoqjLjjytRG1ev2eCsXdqwSGGzbm69//rfuRRYl6ZxeiYqKcPiDB5f7ZAjhO2e5y+tf18HjNVRa7lFpefDhQ1XFZW69/vHGoMtbJH9IcPKfzR8kuNyeOr+vUuU0jU++2aWzurartp1mhNMmayPXNwu3BSVba92I0zUACYlgwPepfUpKihwOR41levToEVC2qT799FNJlYEDAAAAwodvpEJLLRx3uoURdptVdptV0VXeanTtEKNPM3fXG0b85icD6q3XMAx5vEbAKIXyGkKETbuPBTUCIaV9tKIi7apwe6v/81R+9dctyeX2ytXEKUofLq35PYtF8u9ecWpoUPV+VMTJxyNPLDj59sItddbJ7hahV2dLColgoKCgQJLUrl3tG974jvnKNsXixYv15ZdfymKx6O67727y+aTKeSFtnW8OSkPmogBV0YdgBvoRzEA/ghnO6d1B8fFRKiwslcfTvGtVXNS/sy7om6wte44p/7hLCbFOpfZIbLYw4qL+nWW1WfXOwi3KqxpGxEfo9itTdUGa+W907rgqVVP/nVXr8duvSpXTaQvqXA5JkRF1v5VJio8IKhj42dVp6tszqdbjhmH4A4JT/7ncnoD7u/YX6qNlu+qtM6VDjCwWqazco1JX5UKVXsOQIfl3ysg/7qr3PA2RV1Su+5//nyKddtmslhMBTuVXm80qu9VS+dV28pjNWrVMla9Wq2xW6bMVe+qsc9bCLWrfLlIRTnvl6Apb4GiL+qZm1GTV5rrDiPtvTje9/7ZGnVW1xO+0kAgGyssrX6xqGy0gSU6nM6BsY+3YsUMPP/ywJOlnP/uZBg0a1KTzSZWpbGJiTJPP01Li45k+gaahD8EM9COYgX4EM7RkP/pB+9gWq+vKob10+UU9tXHnUeUVlikpPlL9zmwvWzOFEVcO7aXYmAi99uH3OlpQ5n+8Q0KU7hl5jn6QnmJqfRe1i1b7jzcG1HWqDglRuui8bqZds8draNn6A/XW+fJDlwfUaRiGyis8Ki13q7TMrZJyd+DtsgqVlrtVUuYO+Fp5u0JH8kt1pI46fZoy7aIxCksq9NjMb2s9brFIDrtvN4vKaRn+ryd2uHBU+eqwWbUsq+6wZ8Z/NsnhsMvprJziYbdZZbdb/CNZHHar/7bv2Mly1cMKj9fQ7Hqmwby7eJsuv6hns/2349Ocr0UhEQxERERIkioqal/J1OVyBZRtjP379+vuu+9WUVGRLr30Uj344IONPldVXq+hwsISU87VnGw2a4ul4jg90YdgBvoRzEA/ghnCpR91ax+lbu0r33AUFjTv36x9u7fT3++9uMaREceOmb+L121XnF3nKIVbR/Qx/ZqbWmeU3aIou0OKqf1D0VNt2pWnKf/8rt5yd1/XVz06x8njqdxm0+0x5PFWfvXfD3j85DGPx5Dbe7JMzuFibdp9rN4646IcslotAdMzfAxDJ6Z+eKT6d+0MyvGSCj37bv3fi7pUHUkhw1BxmbvO8kfyS7Vi3b46R540RWNei+Ljo06/XQmCmSYQzHSDuhw+fFhjx45Vbm6uLrzwQk2dOrXOEQoNFUpbJXk83pBqL9oe+hDMQD+CGehHMAP9yHx9uiX4b3u9hrze5tlEceBZHepcv2HgWR1M/9m2Rp29U9oFtaDkkH6dTd1qM5hgYPyocwIWEPUalcFCTWs51HTfXeX2jtwCrdx0qN46UzrEKDbSLrf3ZKhR4fH6Qw+3xyu392QIcqrKMh5JwY+uOFpY1uyvE835WhQSwUDPnj0lSbm5uaqoqKjxDfuePXsCyjbE0aNH9bOf/Uy7du3SwIED9corrzRp5AEAAACAtqGlt72sWmdLLWDZlrfaPLt7QsBjVotFVrtNDntw60lU1X13bFDBwB1XnB30bia+xSxPHRHhGy2xbV++3lpQ98KOkpQQE9rvH0NiRZ6+ffvK4XDI5XIpK6vmYTmrV6+WJA0YMKBB587Pz9fPf/5z7dixQ/3799frr7+umJjQWQ8AAAAAQN18214O6ddZaWc036KOp9bZt2eSLh3UTX17JjV7nb7dLRLjAt+gJsVFNMvuAL4woi7NFUbUpaYwoi4WS+W0gQinTTGRDrWLcSopPlKdEqOV0iFGP0xPMb3OtigkRgzExsZq2LBh+vLLLzV37txqWwju2rVLmZmZkqSMjIygz3v8+HHddddd2rJli84++2y9+eabiouLM7XtAAAAANASWnp0xOm21WZbqbM1hEQwIEkTJkzQkiVLNH/+fA0aNEijR4+WxWLRoUOHNHHiRHm9Xo0YMUJpaWkBzxs+fLgk6fe//31AaFBaWqpx48Zpw4YNOvPMMzVz5kwlJgY33AQAAAAA2iLf6IiWcrqHEa1VZ0sLmWAgPT1dDz/8sJ5++mlNnjxZ06ZNU2JiorZv3y6Xy6VevXrp8ccfr/a8nJwcSVJJSeDKn2+//bZ/+oEk3XfffbXW/cILL6hjx44mXQkAAAAAnD5O9zCitepsSSETDEjS2LFjlZqaqunTpysrK0tHjx5VSkqKMjIyNG7cuAatDeDb3lCSdu7cWWfZ8vLaF9QAAAAAALSslg4jWqvOlmIxDKN59gaBn8fjVV6e+fuyms1utyoxMUbHjhWzJQ8ahT4EM9CPYAb6EcxAP0JT0Ydghsb0o6SkGNlswe81EBK7EgAAAAAAgOZBMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxiyGYRit3YjTnWEY8npD49tss1nl8XhbuxkIYfQhmIF+BDPQj2AG+hGaij4EMzS0H1mtFlkslqDLEwwAAAAAABDGmEoAAAAAAEAYIxgAAAAAACCMEQwAAAAAABDGCAYAAAAAAAhjBAMAAAAAAIQxggEAAAAAAMIYwQAAAAAAAGGMYAAAAAAAgDBGMAAAAAAAQBgjGAAAAAAAIIwRDAAAAAAAEMYIBgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwpi9tRuA1peZmakZM2Zo3bp1KikpUUpKijIyMjRu3DhFR0e3dvPQxk2dOlUvvvhinWX+/Oc/69Zbb22hFqEtOnz4sJYtW6b169fr+++/16ZNm1ReXq4LL7xQs2bNqvO5FRUVeuutt/TRRx9pz549cjgcSktL009/+lNdeeWVLXQFaAsa24+GDx+unJycOs+dlZWliIgIs5uMNsYwDK1Zs0ZffPGFVq9erZ07d+r48eOKi4tTv379NGrUKF1//fWyWCw1Pr+4uFivvfaaFi5cqNzcXEVHR+u8887TXXfdpYsuuqiFrwatoSl9KDU1tc5zd+jQQcuWLWuupqON+eyzz7R8+XJt2LBBhw4dUn5+vhwOh3r27KlLL71UP/vZz5SYmFjjc5vjtYhgIMzNmjVLTz75pAzDUOfOndWlSxdt375d06ZN06JFizR79mwlJCS0djMRAtq3b68zzjijxmMdO3Zs4dagrfn00081ZcqUBj+vvLxcP//5z7V69WrZbDadddZZKi0t1cqVK7Vy5Urdc889evDBB5uhxWiLGtuPfM4++2zFxsbWeKy2N4I4vWRmZmrs2LH++927d1fXrl2Vk5OjZcuWadmyZfr00081depUOZ3OgOfm5eXptttuU3Z2tpxOp8466yzl5eVpyZIl+uqrrzRp0iTdfvvtLXxFaGlN6UM+55xzTo3H+Js7vLzyyivavHmznE6nOnbsqNTUVOXl5Wnjxo3auHGj5s6dq+nTpystLS3gec31WkQwEMbWr1+vp556SpL02GOPafTo0bJYLDp48KDGjx+vDRs2aNKkSZo6dWortxSh4JJLLtHTTz/d2s1AGxUbG6sf/OAHOvfcc3Xuuedq48aNevnll+t93t/+9jetXr1a3bp10+uvv64zzzxTkvT555/rN7/5jV5//XUNGjRIw4cPb+5LQBvQ2H7k86c//YlPdcOcYRjq1q2bfvazn+naa69V+/bt/cc+/PBDTZo0SUuWLNHzzz+v//f//l/Ac//4xz8qOztb/fv317Rp05ScnCzDMDR37lxNnjxZTz75pAYNGqS+ffu29GWhBTWlD/k8//zz6tatW0s1GW3U7bffrl69emnAgAFyOBz+x7ds2aIHH3xQW7du1e9+9zt9+umnAc9rrtci1hgIYy+//LK8Xq9GjhypMWPG+D8tSU5O1rPPPiur1apFixZp8+bNrdxSAKHu5ptv1owZMzRx4kRdccUVAX9I1ebIkSP617/+JUl68skn/aGAJF1++eW6++67JaneqSw4fTSmHwFVpaena8GCBbrzzjur9Z9Ro0bp3nvvlST9+9//ltfr9R/buHGjvvjiC1mtVj333HNKTk6WVDnSZMyYMRo5cqQ8Hk+DgiqEpsb2IeBUo0eP1gUXXBAQCkiVU06efPJJSdL27du1Y8cO/7HmfC0iGAhTxcXFWrp0qaTKTnmqnj17asiQIZKkBQsWtGjbAECSvvjiC1VUVAS8HlV1yy23SJI2bNigPXv2tHTzAISg2NjYan+EV3XJJZdIkvLz85WXl+d/fOHChZKkIUOG1DhtbsyYMZKkr776SiUlJWY2GW1MY/sQ0BBVPwwpLS31327O1yKmEoSpTZs2yeVyyel0Kj09vcYygwcP1vLly7Vu3boWbh1C0ebNm/W73/1Ohw8fVkxMjFJTU3XttdeqT58+rd00hKi1a9dKqnwtqklycrK6deumffv2ae3aterRo0cLtg6h6F//+pemT5+usrIydejQQeeff76uv/76WtcdQPgpKyvz346MjPTf9r0enX/++TU+Lz09XU6nU+Xl5dq0aVOtr1s4/dXWh6p6+eWXdejQIXk8HiUnJ2vIkCG65ppral2TAOFn9erVkqTo6Gj16tXL/3hzvhYRDISp7OxsSVJKSkqtqafvj2xfWaAumzZt0qZNm/z3v/jiC73yyiu688479dBDD8lms7Vi6xCKdu3aJUl1vuHv0aOH9u3bx+sUgvKf//wn4P4nn3yi559/Xn//+9918cUXt1Kr0Jb45vKmpaUFBEb1vR45HA516dJFu3fvVnZ2NsFAGKutD1X1/vvvB9yfN2+eXnjhBU2dOlX9+/dv9jaibfJ6vf7dd5555hlJ0oMPPqiYmBh/meZ8LSIYCFMFBQWSpHbt2tVaxnfMVxaoSadOnfTAAw/ohz/8obp166bY2FhlZ2dr9uzZ+te//qW33npLdrtdv//971u7qQgxDXmdKiwsbJE2ITRdeOGFGjJkiM4991ylpKSooqJCq1ev1gsvvKCNGzdq/Pjxevfdd/mDPMytX7/ev67JuHHjAo7xeoRg1NWHpMr1cUaOHKm0tDR17txZxcXF+uabb/Tcc89p7969uuuuu/Thhx+qS5cuLd10tKKZM2dW23EnPT1dTz/9tH9qik9zvhaxxkCYKi8vl6Q650j5hjP5ygI1GTNmjO69916lp6crKSlJTqdTqampevTRR/3byL311lvat29fK7cUoaYhr1NVh24Cp3r66ac1atQo9e7dW1FRUYqPj9dll13mDwPKy8v1t7/9rbWbiVZ05MgR3X///XK73briiit07bXXBhzn9Qj1qa8PSZVTCK666iqdccYZioiIUFJSkq699lrNnTtXKSkpys/PZ0HdMJScnKxBgwbpvPPOU8eOHWWxWLRp0ybNnz+/2pv75nwtIhgIUxEREZKkioqKWsu4XK6AskBD3XXXXerUqZPcbre++OKL1m4OQkxDXqdqm8cJ1CUyMlK/+c1vJEkrVqxghFyYKioq0j333KPc3Fz179+/xq13eT1CXYLpQ3VJSkryjzBYvHixDMNojmaijbr66qv17rvvau7cufr666/14Ycf6rzzztMnn3yiO++8Ux6Px1+2OV+LCAbCVDDTBIIZqgLUxWaz6bzzzpMk7d69u5Vbg1ATHx8vKbjXKV9ZoKEGDRokqXJu5969e1u5NWhpxcXFuvvuu7Vx40b16dNHb775Zo3zwnk9Qm2C7UP1GThwoKTK3Qzy8/NNbiVCSVpaml599VUlJiZq06ZN/nUrpOZ9LSIYCFM9e/aUJOXm5taaOPm2//KVBRrDN9TJ7Xa3cksQanyvPXWFSrxOoamqDses+qkMTn+lpaX65S9/qbVr16pnz56aMWOGEhMTayxb3+tRRUWFcnNzA8ri9NeQPlQfXotQVWxsrC688EJJldsy+zTnaxHBQJjq27evHA6HXC6XsrKyaizj2yZjwIABLdgynG62bdsmSercuXMrtwShxvfa891339V4/ODBg/61K3idQmNt3brVf5vXqfBRXl6u8ePHa9WqVeratatmzpypjh071lre9xrj+9voVFlZWaqoqFBERIT69u3bHE1GG9PQPlQf399LERERSkhIMKmVCGW+D9WqBkXN+VpEMBCmYmNjNWzYMEnS3Llzqx3ftWuXMjMzJUkZGRkt2jacPpYsWeL/RcdWYGioyy+/XA6HI+D1qCrfys/9+vXTGWec0dLNw2ni9ddflySdddZZSk5ObuXWoCVUVFTo/vvv1zfffKPk5GS99dZb9a4Cf9VVV0mqXIuipk/q5syZI0m65JJLArYWw+mpMX2oLm63WzNmzJAkDRkyRHY7G8eFu/z8fK1cuVKSAt7gN+drEcFAGJswYYIsFovmz5+vOXPm+Bc6OXTokCZOnCiv16sRI0YoLS2tlVuKtmrbtm2aPHmyNm/eHPC41+vVJ598ot/97neSpMsuu0zp6emt0USEsA4dOmjMmDGSpD/+8Y/auXOn/9gXX3yhN954Q5J07733tkr7EBrefPNNzZo1S8eOHQt4/NixY5o8ebIWLlwoSXrggQdao3loYR6PR7/73e/01VdfqWPHjnrrrbfUvXv3ep/Xv39/XXbZZfJ4PPrtb3+rQ4cOSZIMw9CcOXM0f/58Wa1WjR8/vrkvAa2ssX3omWee0bx583T8+PGAx/fv368HHnhAa9euld1u53damFi5cqVefvnlGnft2rBhg37xi1+oqKhIycnJAR/SNudrkcVg2cuwNnPmTD399NMyDENdunRRYmKitm/fLpfLpV69emn27NlKSkpq7Waijdq0aZNGjRolSUpISFBKSopsNpv27NnjX/jk/PPP17Rp01iMKczt37/f31ekyhVzS0pKZLfbAxZpuvvuu3XPPff475eVlWns2LFas2aNbDab+vTpo5KSEv/aAnfddZceeuihFrsOtK7G9KMnn3xSb7/9tiwWi7p27aqkpCSVlZVp586dcrvdslqtmjhxYkC/w+mramjdtWvXOkeJTJo0Sf369fPfz8vL06233qpdu3bJ6XTqrLPO0rFjx7R//35ZLBb98Y9/1E9/+tNmvwa0rsb2oQkTJujzzz+XzWZT9+7d1a5dOxUVFSk7O1uGYSgiIkJPPPGEbrjhhha5DrSuxYsX+0Ogjh07qlOnTrLZbNq/f78OHz4sqXIbw1dffbXalIDmei1inEqYGzt2rFJTUzV9+nRlZWXp6NGjSklJUUZGhsaNG8dwONSpa9eu+s1vfqO1a9dqx44d2r17t1wul9q1a6dLLrlE1113na677jrZbLbWbipamcfjqXGVZbfbHfD4qXvuRkZG6u2339bMmTP18ccfa9euXXI4HLrwwgt1xx13+IfUITw0ph/59hLPyspSbm6uNm/eLJvNpm7duunCCy/UbbfdxpzwMOLbxkuScnJylJOTU2vZoqKigPtJSUl6//339frrr2vBggXavn27oqOjdckll+gXv/iFhgwZ0mztRtvR2D506623qkOHDlq/fr0OHTqknJwcORwO9enTR0OHDtUdd9yhHj16NGvb0XYMHDhQjzzyiFasWKHt27dr165dcrlcio+P10UXXaThw4fr5ptvrnGHi+Z6LWLEAAAAAAAAYYw1BgAAAAAACGMEAwAAAAAAhDGCAQAAAAAAwhjBAAAAAAAAYYxgAAAAAACAMEYwAAAAAABAGCMYAAAAAAAgjBEMAAAAAAAQxggGAAAAAAAIYwQDAAAgbKWmpio1NVUrVqxo7aYAANBq7K3dAAAA0HZMnTpVL774YtDlt2zZ0oytAQAALYFgAAAA1KhDhw6t3QQAANACCAYAAECNli1b1tpNAAAALYA1BgAAAAAACGOMGAAAAKYYPny4cnJyNGXKFF155ZV69dVXtWjRIu3fv19RUVEaPHiwfvnLX+q8886r9Rwej0fz5s3TRx99pC1btqi4uFiJiYkaOHCgbr/9dl100UV1tmH//v2aNWuWli1bpn379qmiokKdOnVSnz59dNVVV+nqq69WREREjc89fvy4Xn/9dS1cuFC5ubmKiorSgAEDNGHChDrbDABAqCMYAAAApiosLNTNN9+s7OxsORwORUREKD8/X59//rm+/PJLPf7447r55purPa+oqEgTJkzQypUrJUk2m00xMTE6fPiwFi5cqIULF+quu+7SQw89VGO9H374oSZPnqzy8nJJksPhUExMjPbv36+9e/fqiy++UGpqqvr27VvtuYcPH9ZNN92k3bt3KyIiQlarVfn5+VqyZImWLVumV155RcOGDTPxuwQAQNvBVAIAAGCqF198UXl5efrHP/6htWvXavXq1frPf/6jCy+8UF6vV//3f/+nDRs2VHveH//4R61cuVIOh0N/+tOftHr1aq1atUpLly7Vj3/8Y0nS9OnT9e6771Z77pIlS/Twww+rvLxcgwYN0jvvvKOsrCytWLFCa9as0TvvvKPRo0fL4XDU2ObHHntMDodDb731ltauXas1a9bovffeU69evVRRUaHJkyfL6/Wa+40CAKCNsBiGYbR2IwAAQNtQdbvC+nYluPrqq/WnP/3Jf983lUCSZs6cqaFDhwaULysr08iRI7Vr1y5deumleu211/zH1q1bp9GjR0uqfJM+ZsyYavU98MADWrhwoRITE/XVV1/5pwS43W5dddVV2rdvnwYPHqyZM2fK6XQGdb2pqamSpKSkJH3yySdq3759wPEtW7bohhtukCTNnj1bgwcPDuq8AACEEkYMAACAGh05cqTOf8ePH6/xeYMGDaoWCkhSZGSkfvGLX0iSli5dqqKiIv+x//z/9u4nFLY3juP4Z8zNlGkmhJFYTVkIRWqEUrNgYSNFaTai2MhsbJQpJY01KxtJFoqSzW2ytxiZLIyFMlZjUP7kT2SI32J+Tm7G9ee63eue92t15jzneXrO2c3nPOf7fP8uSSosLFR7e3vacf1+vyTp9PT0hx0TwuGw4vG4JGloaOjNocBTHR0dz0IBKRUcFBcXS0qFBAAA/IuoMQAAANL66B/h2traV9vu7++1tbVl/I5Go5Ikj8ejjIz07y3cbrdcLpcODw8VjUbl9XolSRsbG5Kk/Px8VVRUfGjOPysuWFBQoHg8rrOzsw+NDQDA344VAwAA4FO5XK43tZ2cnBjHx8fHr/aVUisKnl4vpQoHSlJRUdH7J/s/u93+Ytu3b6n3KHd3dx8eHwCAvxnBAAAA+NIsFsufngIAAF8awQAAAPhUh4eHb2rLzc01jh+/7z84OPjp2I/tT+sBPBZJTCQS758sAAAgGAAAAJ8rHA6/2paRkaGysjLjfHl5udH+0raAsVjMCBae1hKorq6WlPqkYHNz89cmDwCACREMAACATxWJRNKGAzc3N5qenpYkNTQ0yOl0Gm0tLS2SUisKFhYW0o47MTEhScrJyVFdXZ1x3uPxqKSkRJIUDAaVTCY/50YAADAJggEAAPCpHA6HBgYGFAqFjIJ9sVhMvb292t3dldVq1cDAwA99Kisr1dzcLEkaHR3V3Nycrq+vJaVWAgwPDysUCklKbVtos9mMvlarVYFAQBaLRZFIRF1dXVpfXzdWHiSTSYXDYQ0ODmpnZ+e33z8AAF8N2xUCAIC06uvrX71mcnLSWMr/qL+/X/Pz8/L7/crMzJTNZtPFxYWkVKHAkZGRtNsKjo2N6fT0VGtraxodHVUwGJTdbtf5+bkeHh4kSd3d3ers7HzWt7GxUePj4woEAopEIvL5fMrMzFRWVpYuLy+NgKKnp+fdzwEAgH8dwQAAAEjr6Ojo1Wtub2+fnXM6nVpcXNTU1JRWVla0v7+v7OxsVVVVqa+vT1VVVWnHcjgcmpmZ0dLSkpaXl7W9va2rqyvl5eWpurpaPp9PHo/nxbm0traqpqZGs7OzWl1dVSKR0M3NjYqKilRaWqqmpia53e63PwAAAEzC8vAYwQMAAPwCr9ervb09BYNBtbW1/enpAACAN6LGAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMUHAQAAAAAwMVYMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGL/AU5U4BallF45AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "plt.plot(val_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rKkmqhnO7WEj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKOXI_p47WGZ",
        "outputId": "88f1a217-3c3f-40fb-d84f-b3eccfd8421e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd348652e95b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Report the number of sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FILTER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Report the number of sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dat' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Report the number of sentences.\n",
        "print(dat.groupby('FILTER').size())\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(dat.shape[0]))\n",
        "\n",
        "\n",
        "\n",
        "# Get all column names except the one to exclude\n",
        "columns_to_merge = [col for col in dat.columns if col != 'FILTER']\n",
        "\n",
        "# Define a lambda function to merge the values of selected columns\n",
        "merge_columns = lambda row: ' '.join(str(row[col]) for col in columns_to_merge)\n",
        "\n",
        "# Merge columns using the defined function\n",
        "dat['merged_column'] = dat.apply(merge_columns, axis=1)\n",
        "\n",
        "# Drop the original columns\n",
        "dat = dat.drop(columns=columns_to_merge)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(dat.sample(3))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = dat.merged_column.values\n",
        "labels = dat.FILTER.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 1000000\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igsCeuiB68PI"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9J3wWlc0XGa"
      },
      "outputs": [],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ND01k8w0XI5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y273is9J0XLQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GKD0EW30XNU"
      },
      "outputs": [],
      "source": [
        "matthews_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV7KfuZb0XQu"
      },
      "outputs": [],
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWrmfDJW0wJl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdXYVb9b0wMI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xORBeqF_0wOf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz6lD0fR0wQZ"
      },
      "outputs": [],
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KDqYLiK0wTz"
      },
      "outputs": [],
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znu4g8M11JoF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmVfsf8x1JqZ"
      },
      "outputs": [],
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/MyDrive/Colab\\ Notebooks/deepref/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8ZYrsmY1Jsv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tFycQQU1JwB"
      },
      "outputs": [],
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "#model = model_class.from_pretrained(output_dir)\n",
        "#tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "#model.to(device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}